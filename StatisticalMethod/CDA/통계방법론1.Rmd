---
title: "통계 방법론1 중간대체"
author: "응용통계학과 이준혁"
date: "2023-10-28(토)"
output:
  html_document:
    css: styles.css
    #code_folding: show
    fig_caption: yes
    fig_height: 7.5
    fig_width: 10
    fig_retina: null
    highlight: haddock
    self_contained: yes
    theme: cosmo
    toc: yes
    toc_depth: 6
    toc_float: yes
    fig_dpi: 300
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style type="text/css">
  body, td {
     font-size: 16px;
     font-family: 맑은 고딕
  }
  code.r{
    font-size: 16px;
    font-weight: bold;
    font-family: 맑은 고딕
  }
  pre {
    font-size: 14px
    font-family: 맑은 고딕
  }
  h1,h2,h3,h4,h5,h6{
    font-family: 맑은 고딕;
    font-weight: bold;
  }
  h1{
    font-size: 18pt;
  }
  h2{
    font-size: 16pt;
  }
  h3{
    font-size: 14pt;
  }
  table{
    font-size: 20px;
  }
</style>
<br><br><br><br>

---

# !목차

## Is it not time to seek out novelty in data analysis?
-   생각하기
-   Part2 Statistical Method - 요약, 비교, 관계
  -   1. 통계적 의사 결정 Statistical Decision Making
  -   2. 확률 분포와 그 응용
  -   3. 확증적 비교 방법 Confirmatory Comparisons
  -   4. 단순 회귀 분석 Simple Regression Analysis
  -   5. 다중 회귀 분석 Multiple Linear Regressionwwww
  
---


# 생각이 필요한 개념 및 주제.

-   자유도
-   통계적 의사 결정
    -   모든 관심 특성은 **[변수]{style="color: red;"}**이다.
    -   모든 통계량은 고유의 **[표본 분포(sampling distribution)]{style="color: red;"}**를 갖는다.
        -   이는 표본 추출에 의하여 발생하는 **[표본 오차(sampling error)]{style="color: red;"}**를 수량화한다.
    -   표본 오차(sampling error)가 존재한다면, 이를 **[의사결정에 반영]{style="color: red;"}**할 수 있어야 한다.
-   연속형일 때는 측정 단위가 매우 중요하다 -> 표준화 적용
-   다변량에서 차원 축소를 진행할 때 y값을 다 표준화하는 게 추세이다.
-   쌍체 검정은 쌍별 차이를 고려하기 때문에 개체 내의 변동성은 제거하고 측정값 간의 차이에만 집중을 할 수 있다.

# ML 적용 과정 생각하기

P19,

-   heuristics에 의해서 $\alpha=0.05$로 설정하는 것처럼 머신러닝에서는 heuristics에 의해 선택되는 것이 많이 있다.
-   A/B test 방법 중에 하나가 T-test로 봐도 되는 것인가?
-   기본Base 모델인 A모델과 B모델을 비교하고자한다.
-   모델 선택도 heuristics에 의해서 좋은 모델을 선택한다.
    -   $$H_0 : \mu_B \leq \mu_A$$, B모델의 성능은 A모델의 성능보다 나쁘거나 같다.
    -   $$H_1 : \mu_B > \mu_A$$, B모델의 성능은 A모델의 성능보다 좋다.
    -   같은 샘플을 가지고 ML모델을 적용해서 T-test를 통해서 모델을 비교한다.(대응 T-test)

# 1. 통계적 의사 결정 Statistical Decision Making

## 1.1. 모수와 통계량

-   **모수:** 모집단의 특징으로 **[상수(constant)]{style="color: red;"}**
-   **통계량:** 표본에서의 결과이며 **[변수(variable)]{style="color: red;"}**

```{r}
unit1 = c(10, 10, 10, 50, 30, 40, 50, 40, 30 ,50)
unit2 = c(20, 30, 40, 10, 20, 20, 20, 30, 50, 40)

results <- ((unit1 + unit2) / 2) %>% print()

# 결과를 데이터 프레임으로 변환
df <- data.frame(Sample.Mean = results)
df

# 도트 플롯으로 시각화하기
ggplot(df, aes(x = Sample.Mean)) + 
  geom_dotplot(binwidth = 0.5, stackdir = "center", color=sbl, fill= sbl) +
  labs(title="모평균과 표본 평균(n = 2)", x="표본 평균", y="빈도") +
  ylim(0, 1) +
  theme_bw() + 
  geom_vline(xintercept=30, linetype="dashed", color = lre)
```

-   모든 통계량은 고유의 분포를 갖는다. 이를 **[표본 분포(sampling distribution) ]{style="color: red;"}**라고 한다.
-   모든 모집단의 평균이 의미가 있는가?
    -   평균만으로는 분포의 형태, 이상치의 영향, 데이터가 가지는 의미를 파악하기에는 어려움이 있어 보인다.
  
```{r}
# 1. Normal Population Distribution
set.seed(123)
population1 <- rnorm(1000, mean=100, sd=5)
sample_means1 <- replicate(1000, mean(sample(population1, size=30)))

p1 <- ggplot(data.frame(x=population1), aes(x)) + geom_density() + xlim(80, 120)
p2 <- ggplot(data.frame(x=sample_means1), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(97, 103)
gridExtra::grid.arrange(p1, p2, ncol = 2)
```
```{r}
# 2. Gamma Distribution
population2 <- rgamma(1000, shape=1, scale=100)
sample_means2 <- replicate(1000, mean(sample(population2, size=30)))

p3 <- ggplot(data.frame(x=population2), aes(x)) + geom_density() + xlim(0, 300)
p4 <- ggplot(data.frame(x=sample_means2), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(50, 150)
gridExtra::grid.arrange(p3, p4, ncol = 2)

```
```{r}
# 3. Exponential Distribution
population3 <- rexp(1000, rate=0.005)
sample_means3 <- replicate(1000, mean(sample(population3, size=30)))

p5 <- ggplot(data.frame(x=population3), aes(x)) + geom_density() + xlim(0, 1000)
p6 <- ggplot(data.frame(x=sample_means3), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(0, 600)
gridExtra::grid.arrange(p5, p6, ncol = 2)
```

###  대수의 법칙 Law of Large Numbers

```{r}
# 데이터 생성
n <- 10000

# Normal(100, 10)
normal_1 <- rnorm(n, mean=100, sd=10)
cum_normal_1 <- cumsum(normal_1) / (1:n)

# Normal(90, 20)
normal_2 <- rnorm(n, mean=90, sd=20)
cum_normal_2 <- cumsum(normal_2) / (1:n)

# Uniform(40, 120)
uniform_data <- runif(n, min=40, max=120)
cum_uniform <- cumsum(uniform_data) / (1:n)

# 그래프 그리기
plot_df <- data.frame(
  Iteration = 1:n,
  Normal_100_10 = cum_normal_1,
  Normal_90_20 = cum_normal_2,
  Uniform_40_120 = cum_uniform
)

ggplot(plot_df, aes(Iteration)) +
  geom_line(aes(y = Normal_100_10, color = "Normal(100, 10)")) +
  geom_line(aes(y = Normal_90_20, color = "Normal(90, 20)")) +
  geom_line(aes(y = Uniform_40_120, color = "Uniform(40, 120)")) +
  labs(y = "표본 평균", x = "표본 크기", title = "[LLM] 분포의 형태와 산포 그리고 표본 크기에 따른 표본 평균 값의 변화") +
  theme_bw() +
  scale_color_manual(values = c(sbl, lre, lgo))
```


-   산포가 작고 정규분포일 때 수렴이 더 빠른 것을 알 수 있다.
-   $n_0<n_1$이어도 항상 $n_0$에서의 요약값보다 참값에 근접하지는 않다.

### 중심 극한 정리 Central Limit Theorem; CLT

-   **[모집단 분포와 관계없이,]{style="color: red;"}** 표본 크기(n)가 증가함에 따라 표본 평균의 분포는 정규 분포로 근사한다.
-   표본 크기(n)가 커질수록 표본 평균의 분산이 작아진다.

    -   표준 오차(Standard Error; SE)
        -   통계량의 **[표준 편차]{style="color: red;"}**
        -   표본 오차를 수량화한 것으로 **[그 값이 작을수록 바람직]{style="color: red;"}**
$$SE(\bar{y}) = \frac{\sigma}{\sqrt{n}}$$



### 중요 모수와 통계량

|  | 모수 | 통계량값 | 정의범위 |
|:------:|:----:|:-------:|:--------:|
| 비율 - 이산형 | $p$ | $\hat{p} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = 0,1$ |
| 평균 간격 - 이산형 | $\lambda$ | $\hat{\lambda} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = 0,1,2,...$ |
| 평균 - 연속형 | $\mu$ | $\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = [-\infty, \infty]$ |
| 분산 | $\sigma^2$ | $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2$ | $\forall y_i$ |

$$\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i = \frac{1}{n}y_1 + \frac{1}{n}y_2 + \dots + \frac{1}{n}y_n$$

-   **[표본 평균의 평균]{style="color: red;"}**은 모평균이다. 
    -   $E(\bar{y}) = \mu$
-   정보 자체는 연속형이 더 많이 내포하고 있기 때문에 어떻게 하면 이산형 변수를 근사화할 수 있지에 대한 고민이 중요하다.
-   예를 들어 사람의 키가 170.5cm라는 값이 있는데 이를 '키가 크다'라고 분류해버리면 이는 정보의 손실에 해당한다.

## 1.2. 중요 통계량의 표본 분포

-   표본 분포는 (1)표본 크기(n)의 영향을 받고 (2)통계량의 표본 오차를 표현한다.

### 카이 제곱 분포 $\chi^2$

-   카이 제곱분포
    -   표준정규분포 제곱합(Sum of Squared)의 분포
    -   $Z_i \sim N(0,1)$, i = 1, 2, ..., v가 상호 독립일 때
    -   $C = \sum_{i=1}^{v} Z_i^2 \sim \chi^2(v)$, $v$는 자유도
    -   $E(C) = v$
    -   $\text{Var}(C) = 2v$
    -   평균이 커지면 분산이 커지는 분포이다.
    
* 두 표본이 서로 독립이라면, $P(X=x, Y=y) = P(X=x)P(Y=y)$

-   카이제곱 분포의 **[가법성]{style="color: red;"}**
    -   가법성 개념이 중요하다.
        -   계산이 편리해진다. -> 해석이 용이해진다.
        
$$C_1 \sim \chi^2(v_1),~C_2 \sim \chi^2(v_2)~\text{상호 독립이라면,}$$

$$C_1 + C_2 \sim \chi^2(v_1 + v_2)$$

-   실무적으로 상호 독립을 설명하기가 어렵다.
    -   랜덤하게 추출되었으면 상호독립일 가능성이 높다.
    -   **[교차표]{style="color: red;"}**를 정리해서 검증하면 좋다.

```{r}
# x 축을 위한 데이터 벡터 생성 (0부터 80까지)
x <- seq(0, 80, by = 0.1)

# 자유도가 4, 15, 29, 39, 49인 카이제곱 분포의 확률밀도함수 계산
y_df1 <- dchisq(x, df = 4)
y_df2 <- dchisq(x, df = 14)
y_df3 <- dchisq(x, df = 29)
y_df5 <- dchisq(x, df = 39)
y_df10 <- dchisq(x, df = 49)

# 데이터 프레임 생성
df <- data.frame(x, y_df1, y_df2, y_df3, y_df5, y_df10)

# 그래프 그리기
ggplot(df, aes(x = x)) +
  geom_line(aes(y = y_df1, color = 'df = 4')) +
  geom_line(aes(y = y_df2, color = 'df = 14')) +
  geom_line(aes(y = y_df3, color = 'df = 29')) +
  geom_line(aes(y = y_df5, color = 'df = 39')) +
  geom_line(aes(y = y_df10, color = 'df = 49')) +
  ggtitle("자유도에 따른 카이제곱분포의 형태") +
  xlab("x") +
  ylab("Density") +
  scale_color_manual(values = c(lre, sbl, ngr, 'purple', lgo)) +
  theme_bw()
```

-   고려 사항
    -   언제 사용하는가?
        -   (1)모분산의 통계적 추론
        -   (2)범주형 변수의 **[독립성/동일성 검정]{style="color: red;"}**
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code
    
```{r}
qchisq(0.025, df = 49) #31.6
qchisq(0.975, df = 49) #70.2
```

---

### t분포 $t_{\nu}$

- 독립변수 $Z$ 와 $C$ 가 각각 $N(0,1)$, $C \sim \chi^2(v)$ 인 경우를 가정하면,

$$ T = \frac{Z}{\sqrt{C/v}}~\sim t(v) $$

- $E(T) = 0$
- $Var(T) = \frac{v}{v-2}, v > 2$

---

-   Why t ?

$$ \bar{y} \sim N \left( \mu, \frac{\sigma^2}{n} \right) $$

$$ Z = \frac{\bar{y} - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1) $$

$$ \frac{(n-1)s^2}{\sigma^2} \sim  \chi^2(n-1) $$

따라서,

$$ T = \frac{Z}{\sqrt{\frac{(n-1)s^2}{\sigma^2(n-1)}}} = \frac{\bar{y} - \mu}{\sqrt{s^2/n}} \approx t(n-1) $$


-   t-분포와 정규분포 비교

```{r}
# x 범위 정의
x <- seq(-5, 5, by=0.01)

# 표준 정규 분포
pdf_norm <- dnorm(x, mean=0, sd=1)

# t-분포 (자유도 3, 10, 25)
pdf_t3 <- dt(x, df=3)
pdf_t10 <- dt(x, df=10)
pdf_t25 <- dt(x, df=25)

# 데이터프레임 생성
df <- data.frame(x, pdf_norm, pdf_t10, pdf_t3, pdf_t25)

# 그래프 그리기
ggplot(df, aes(x=x)) +
  geom_line(aes(y=pdf_norm, color='Normal Distribution')) +
  geom_line(aes(y=pdf_t10, color='t-Distribution (df=10)'), linetype="dashed") +
  geom_line(aes(y=pdf_t3, color='t-Distribution (df=3)'), linetype="dotted") +
  geom_line(aes(y=pdf_t25, color='t-Distribution (df=25)'), linetype="twodash") +
  labs(title="Standard Normal vs. t-Distributions",
       x="Value",
       y="Density",
       color="Distribution") +
  theme_bw()
```

-   오른쪽과 왼쪽 꼬리가 두껍다. 정규분포에 비해 변동성 증가한다.
    -   why? t분포는 작은 표본 크기에서의 표본 평균의 분포를 설명하기 때문에 이러한 특성을 가진다.
-   고려 사항
    -   언제 사용하는가?
        -   모분산을 모르는 경우, 모평균에 대한 통계적 추론
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code
    
```{r}
qt(0.025, df = 10) # 결과: -2.23
qt(0.025, df = 30) # 결과: -2.04
qnorm(0.025) # 결과: -1.96
qt(0.975, df = 10) # 결과: 2.23
qt(0.975, df = 30) # 결과: 2.04
qnorm(0.975) # 결과: 1.96
```
-   이를 통해서 n이 커짐에 따라서 정규 분포로 근사하는걸 알 수 있다.

---

### F분포 $F_{\nu1, \nu2}$

F-분포는 두 개의 자유도 $\nu_1$과 $\nu_2$를 가진 카이제곱 분포의 비율에 대한 분포로 정의된다.

$F = \frac{C/\nu_1}{U/\nu_2}$


$E(F) = \frac{\nu_2}{\nu_2 - 2}, \quad \nu_2 > 2$

$Var(F) = \frac{2\nu_2(\nu_1+\nu_1)}{\nu_1(\nu_2-2)^2(\nu_2-4)}, \quad \nu_2 > 4$

-   why F?

$$(n_1 - 1) \frac{s_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$$

$$(n_2 - 1) \frac{s_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$$

$$F = \frac{\frac{(n_1-1)s_1^2}{\sigma_1^2} / (n_1 - 1)}{\frac{(n_2-1)s_2^2}{\sigma_2^2} / (n_2 - 1)} = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2} \sim F(n_1-1, n_2-1)$$




```{r}
# x 범위 정의
x <- seq(0, 5, by=0.01)

# F 분포 자유도 조합 정의
dfs <- list(c(10,10), c(10,30), c(10,5), c(30,10), c(30,30), c(30,5), c(5,10), c(5,30), c(5,5))

# 각 자유도 조합에 대한 F 분포 계산
pdfs <- lapply(dfs, function(df) df(x, df1=df[1], df2=df[2]))

# 데이터프레임 생성
df_data <- data.frame(x = rep(x, length(dfs)),
                      y = do.call(c, pdfs),
                      df = factor(rep(sapply(dfs, paste, collapse="-"), each=length(x))))

# 그래프 그리기
ggplot(df_data, aes(x=x, y=y, color=df)) +
  geom_line() +
  labs(title="F distribution for various degrees of freedom",
       x="Value",
       y="Density",
       color="Degrees of Freedom") +
  theme_bw()
```

-   고려 사항
    -   언제 사용하는가?
        -   (1)모분산의 비에 대한 통계적 추론
        -   (2)분산 분석
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code

```{r}
qf(0.025, df1 = 5, df2 = 30) #0.161
qf(0.975, df1 = 5, df2 = 30) #3.03
```

---

## 1.3. 통계적 추론 Statistical Inference

-   표본 오차가 존재한다면, 이를 의사 결정 과정에 반영할 수 있어야 한다.
-   실무적 고려 사항은 통계적 고려 사항의 전제 조건이다.

### 1.3.1. 구간 추정 Interval Estimation

-   주어진 신뢰도를 만족하는 신뢰구간을 설정하여 미지의 모수 또는 모수들의 함수를 추정하는 방법
    -   95% 신뢰구간을 선택하는 것은 heuristics이다.
    -   표본의 크기가 늘어나면 불확실성이 줄어드는 것이지 참값에 도달한다는 것은 아니다.
        -   **[표본을 적절하게 설정하는 것이 중요!]{style="color: red;"}**
    
$~~~~~~~~(1-\alpha)100\% \text{ 신뢰구간}$

-   모수(도는 모수의 함수) $\theta$와 이에 대한 추정량 $\hat{\theta}$에 대하여 다음을 만족하는 구간 $[L,~U]$

$~~~~~~~~\Pr(L \leq \theta \leq U) = 1 - \alpha, \ 0 < \alpha < 1$

-   실제 모수는 구해진 신뢰 구간 내에 존재한다.
-   가능하면, 최악의 경우를 고려하여, 신뢰 하한(Lower Limit) 또는 신뢰 상한(Upper Limit) 중의 하나로 해석한다.

-   단일 모평균의 구간 추정
$$
T = \frac{\bar{y} - \mu}{s/\sqrt{n}} \sim t(n-1), s^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (y_i - \bar{y})^2
$$

-   $(1-\alpha)100\% \text{ 신뢰 구간}$

$$
\text{Pr} \left[ |T| \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$

$$
\text{Pr} \left[ -t_{\alpha/2}(n - 1) \leq T \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$
$$
\text{Pr} \left[ -t_{\alpha/2}(n - 1) \leq \frac{\bar{y} - \mu}{s/\sqrt{n}} \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$
$$
\text{Pr} \left[ \bar{y} - t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \leq \mu \leq \bar{y} + t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \right] = 1-\alpha \\
$$
따라서
$$
[L, U] = \left[ \bar{y} - t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}}, \bar{y} + t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \right]
$$


### 1.3.2. 가설 검정 -보충
    -   **[증명하고자 하는 주장을 대립 가설로 설정]{style="color: red;"}**한다.
    -   (모집단의 특성에 대한) 어떤 주장이나 추측을 가설로 설정하고 표본 관찬을 통하여 **[이의 기각(rejection) 여부]{style="color: red;"}**를 결정하는 방법

---

## 실습 1. 구간 추정 결과의 해석

-   제품간/프로세스간/업무실적간 개선 전/후의 모평균을 비교하고자 한다.
    -   단, 반응값(y)는 큰 값이 바람직하다.
        -   1) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (-30.5, -10.5)이다.
            -   (1) 0이 미포함되었으므로 개선이 되었다.
            -   (2) 음수가 되었다는 건 $\mu2$으로 개선이 되었다.($\mu2$평균이 더 높다다)
            -   (3) 최소 -10.5 개선되었다. -> **["최악의 경우"]{style="color: red;"}**로 봐야 한다.
                -   -30.5가 개선되었다고 하면 **[과대해석; 과장광고의 느낌]{style="color: #FAB23D;"}**이다.
        -   2) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (-10.3, 10.7)이다.
            -   (1) 0이 포함되었으므로 개선이 되지 않았다고 해석 진행
        -   3) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (10.7, 30.2)이다.
            -   (1) 0이 미포함되었으므로 개선이 되었다.
            -   (2) 양수가 되었다는 건 $\mu1$으로 개선이 되었다.
            -   (3) 최소 10.7 개선되었다. -> **["최악의 경우"]{style="color: red;"}**로 봐야 한다.
        -   따라서 신뢰 구간의 하한을 기준으로 해석하는 것이 합리적인 접근 방식이다.
        
---

## 실습 2. 통계적 추론의 응용 storage.lot.csv

-   TU 에너지 저장 장치는 개별 LoT에서 자동 생산된다. 각 LOT에서는 자동 선별 및 측정을 통하여 일정 수를 대상으로 저장 능력을 검증하고 있으며, 검증 횟수는 LOT별로 일정하지 않다.
-   중요 관리 기준은 각 LOT별 제품의 에너지 저장 능력의 평균이며, 만일 LOT의 평균 생산 능력이 985.0보다 작다면, 해당 LOT는 재점 대상이 된다.  

---

-   데이터 불러오기
```{r}
perform <- read.csv(paste0(data_path, "storage.lot.csv"))
str(perform)
```

-   데이터 시각화

```{r}
limit <- 985

perform %>% 
  ggplot(aes(x = lot, y = storage, group = lot)) +
  geom_boxplot(position = position_dodge(width = 0.8), color = sbl) +
    labs(x = "lot_no", y = "storage") +
    geom_hline(yintercept = limit, color = lre) +
    scale_x_continuous(breaks=seq(1, 100, 1)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    theme(axis.text.x = element_text(size = 6))
```

-  lot별로 요약이 필요해보인다.
    -   재점검 판단이 불가능하다.

---

-   데이터 요약1

```{r}
p_sum <- perform %>% 
  group_by(lot) %>% 
  summarize(lmean = mean(storage), lsd = sd(storage))

head(p_sum, 3)
```

```{r}
p_sum %>% ggplot(aes(lmean, lsd)) +
  geom_point(color = sbl) +
  geom_vline(xintercept = 985, color = lre, linetype = "dashed") +
  theme_bw()
```

-   이렇게 표현을 하게 되면 대부분의 lot가 985.0보다 작아 재점검 대상이 된다.
    -   의미가 없는 시각화이다.
    -   따라서 각 "lot"의 평균값의 신뢰구간을 표현하여 다시 분석을 실시한다.  

-   데이터 요약2: 95% 신뢰구간

```{r}
perform_c <- perform %>%
  group_by(lot) %>%
  summarize(
    avg = mean(storage),
    lower = mean(storage) - qt(0.975, n() - 1) * (sd(storage) / sqrt(n())),
    upper = mean(storage) + qt(0.975, n() - 1) * (sd(storage) / sqrt(n()))
  ) %>%
  ungroup()

perform_c <- as.data.frame(perform_c)

perform_c %>%
  ggplot(aes(x = lot, y = avg)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper, color = ifelse(upper < 985, "A", "B")), width = 0.2) +
  scale_color_manual(values = c("A" = lre, "B" = sbl)) +
  guides(color = FALSE) +
  labs(x = "lot", y = "Average") +
  theme_bw()
```

-   대부분의 "lot"는 평균값 주변에 데이터가 밀집되어 있어 변동성이 크지 않아 보인다.
-   빨간색 바로 표시되는 "lot"는 상한값이 985.0보다 작은 것을 의미한다.
-   따라서3개의 "lot"가 관리 기준을 만족하지 못하는 것으로 파악이 된다.
    -   재점검 실시


---

# 2. 확률 분포와 그 응용

-   확률 이론은 계산으로 축소된 상식일 뿐이다.
-   다음 세가지를 고민하면서 접근해야 어떤 문제를 맞닿았을 때 고민을 해보고 적용할 수 있다.
    -   언제 사용하는가?
        -   해당 확률 분포나 방법이 적절한 상황을 판단하는데 중요하다.
    -   확률 분포를 결정하는 모수(들)은 무엇인가?
    -   관심 사상에 대한 확률을 어떻게 계산할 것인가?

---

-   실무적 관심 특성에 대한 중요 확률 분포

| 분포 유형 | 분포 이름     | 모수          | R 함수   | 기호                |
|----------|--------------|--------------|---------|-------------------|
| 이산형   | 이항 Binomial | n, p         | `binom()` | $Bin(n, p)$       |
|          | 포아송 Poisson| $\lambda$   | `pois()`  | $Poi(\lambda)$    |
| 연속형   | 정규 Normal   | $\mu, \sigma^2$ | `norm()` | $N(\mu, \sigma^2)$ |

---

-   R에서 쓰는 확률 분포 함수들 의미:
$$
d*: \text{확률 밀도(질량) 함수 probability mass(density) function } p(\theta) = f(y; \theta) \\
p*: \text{누적 분포 함수 cumulative distribution function } F(y) = Pr(Y \leq y) \\
q*: \text{역 누적 분포 함수 inverse cumulative distribution function } F^{-1}(p) = \inf\{z : F(z) \geq p\} \\
r*: \text{랜덤 숫자 생성 함수 random number generation function } r_1, r_2, ... r_n \sim iid \ p(\theta) \\
$$

---

## 2.1. 이산형 확률 분포

-   유한 또는 가변 값을 취할 수 있는 확률변수의 확률 분포
-   일정 단위에서 특정 범주의 **[빈도]{style="color: blue;"}**를 수집하는 경우 유용하다.
-   개별 비율 vs 누적 비율
    -   누적 비율로 봐야 한다. -> **[비율은 누적]{style="color: red;"}**한다.

---

### 2.1.1. 이항 분포 Binomial Distribution

-   $Y$ ~ $Bin(n,p)$

-   조건 및 특성:
    -   베르누이 시행을 $n > 1$ 번 독립적으로 시행했을 때 특정 사건의 수
    -   특정 모비율 $p$을 갖는 (무한)모집단으로부터 추출한 단위 표본(n) 중 관심 사건의 수
    -   n 번의 시행에서 매회 성공률이 p, $0 < p < 1$로 동일함을 가정

-   확률(질량) 함수:

$$
f(y) = \binom{n}{y} p^y (1-p)^{n-y}, y = 0,1,2,..., n
$$

-   평균: $E(Y) = np$

-   분산: $Var(Y) = np(1 - p)$

-   분산은 평균의 함수이다.
---

```{r}
# Binomial 분포의 데이터 생성
x <- 0:20
y1 <- dbinom(x, 20, 0.1) # Bin(20, 0.1)의 pdf
y2 <- dbinom(x, 20, 0.5) # Bin(20, 0.5)의 pdf

y1_cdf <- pbinom(x, 20, 0.1) # Bin(20, 0.1)의 CDF
y2_cdf <- pbinom(x, 20, 0.5) # Bin(20, 0.5)의 CDF

data_pdf <- data.frame(x, y1, y2)
data_cdf <- data.frame(x, y1_cdf, y2_cdf)

# pdf plot
pdf_plot <- ggplot(data_pdf, aes(x = x)) +
  geom_line(aes(y = y1, color = "Bin(20, 0.1)"), size = 1) +
  geom_point(aes(y = y1, color = "Bin(20, 0.1)")) +
  geom_line(aes(y = y2, color = "Bin(20, 0.5)"), size = 1) +
  geom_point(aes(y = y2, color = "Bin(20, 0.5)")) +
  labs(title = "pdf of Binomial Distributions", y = "확률", x = "x") +
  theme_minimal()

# cdf plot
cdf_plot <- ggplot(data_cdf, aes(x = x)) +
  geom_step(aes(y = y1_cdf, color = "Bin(20, 0.1)"), size = 1) +
  geom_point(aes(y = y1_cdf, color = "Bin(20, 0.1)")) +
  geom_step(aes(y = y2_cdf, color = "Bin(20, 0.5)"), size = 1) +
  geom_point(aes(y = y2_cdf, color = "Bin(20, 0.5)")) +
  labs(title = "CDF of Binomial Distributions", y = "누적 확률", x = "x") +
  theme_minimal()

#
gridExtra::grid.arrange(pdf_plot, cdf_plot, ncol = 2)
```

-   Bin(20, 0.1) vs Bin(20, 0.5)
    -   빨간색 선은 성공확률의 0.1인 20번의 시도에 대한 분포이다. 0~5사이에 집중되어 있다.
        -   따라서 20번의 시도중에서 0~5번 성공할 확률이 높다는 것을 알 수 있다.
    -   파란색 선은 성공확률의 0.5인 20번의 시도에 대한 분포이다. 10 근처에서 가장 높을 값을 가진다.
        -   즉, 20번의 시도 끝에 10번 성공할 확률이 가장 높음을 의미한다.
    -   누적 확률을 확인해봐도 앞서 봤던 결과처럼 빨간색선은 x = 5일때 누적확률이 거의 1에 가까워지는 것을 알 수 있다.

---

### 2.1.2. 포아송 분포 Poisson Distribution

-   $Y$ ~ $Poi(\lambda)$

-   확률 (질량)함수:
$$f(y) = \frac{\lambda^y e^{-\lambda}}{y!}$$
여기서, $y = 0, 1, 2, ...$

-   기대값: $E(Y) = \lambda$

-   분산: $Var(Y) = \lambda$

-   $Y_i \sim Poi(\lambda_i), i = 1, 2, ..., k$

$$\sum_{i=1}^{k} Y_i \sim Poi(\sum_{i=1}^{k} \lambda_i)$$

-    분산은 평균의 함수이다.

```{r}
# 포아송 분포의 데이터 생성
x <- 0:20
y1 <- dpois(x, 3) # Poi(3)의 pdf
y2 <- dpois(x, 10) # Poi(10)의 pdf

y1_cdf <- ppois(x, 3) # Poi(3)의 CDF
y2_cdf <- ppois(x, 10) # Poi(10)의 CDF

data_pdf <- data.frame(x, y1, y2)
data_cdf <- data.frame(x, y1_cdf, y2_cdf)

# pdf plot
pdf_plot <- ggplot(data_pdf, aes(x = x)) +
  geom_line(aes(y = y1, color = "Poi(3)"), size = 1) +
  geom_point(aes(y = y1, color = "Poi(3)")) +
  geom_line(aes(y = y2, color = "Poi(10)"), size = 1) +
  geom_point(aes(y = y2, color = "Poi(10)")) +
  labs(title = "pdf of Poisson Distributions", y = "확률", x = "x") +
  theme_minimal()

# cdf plot
cdf_plot <- ggplot(data_cdf, aes(x = x)) +
  geom_step(aes(y = y1_cdf, color = "Poi(3)"), size = 1) +
  geom_point(aes(y = y1_cdf, color = "Poi(3)")) +
  geom_step(aes(y = y2_cdf, color = "Poi(10)"), size = 1) +
  geom_point(aes(y = y2_cdf, color = "Poi(10)")) +
  labs(title = "CDF of Poisson Distributions", y = "누적 확률", x = "x") +
  theme_minimal()

#
gridExtra::grid.arrange(pdf_plot, cdf_plot, ncol = 2)
```

-   각각 사건의수가 3일때와 10일 때의 부근에서 많이 일어난 것을 알 수 있다.

```{r}

```


## 2.2. 연속형 확률 분포

-   연속형 확률 변수
    -   임의의 구간 또는 구간들의 합을 취할 수 있는 확률 변수
    -   구간/비율 척도로 측정되며, 유무형의 측정 장비에 의하여 확인이 가능
    -   측정 장비를 적용하는 경우, 고유의 측정 단위 존재

---

-   연속형 확률 분포를 결정하는 요소
    -   1. 기술적: 관심 특성의 기술적 특징
    -   2. 환경적: 데이터의 측정 환경
    -   3. 상식적: 실무 현상에 대한 객관성과 타당성
    
---

-   연속형 확률 분포는 **[형태(shape), 위치(location), 척도(scale)]{style="color: red;"}**에 의하여 결정

---

-   연속형 확률밀도 함수
    -   1) 모든 가능한 y값에 대하여 확률 밀도 값은 0보다 크거나 같다.
    -   2) 모든 가능한 y값에서 구한 면적은 1이다.
    -   3) 관심 사상이 정의된 구간의 면적으로 해당 확률을 계산한다.

---

### 2.2.1. 정규분포 Normal Distributions

-   $Y$ ~ $N(\mu,\sigma^2)$



---

## 실습 3. 이항 분포를 이용한 확률 계산

-   지난 분기 개발 업무(A)d의 일정 미준수율은 약 20%정도이고 검증 업무 (B)의 시간 미준수율은 약 10%정도이다. 차기 분기 이전에 각각의 업무에서 임의로 선택된 30건에 대한 미준수 건수를 조사하여 개선 프로세스 도입 여부를 판단하고자 한다. (A)와 (B) 각 업무에 대하여 다음을 계산하시오.
    -   1. 30건 중에서 미준수가 1건도 없을 확률
        -   (A)의 경우: $P(X=0) = \binom{30}{0} (0.2)^0 (0.8)^{30}$
        -   (B)의 경우: $P(X=0) = \binom{30}{0} (0.1)^0 (0.9)^{30}$
    -   2. 30건 중에서 미준수가 6건 이상 발생할 확률
        -   (A)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=5))$
        -   (B)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=5))$
    -   3. 미준수 건수가 4건 이상일 확률이 50%를 넘지 않는다면 현재 프로세스를 유지하고, 만일 50%를 넘는다면 새로운 관리 기준을 도입하려 한다. 두 업무는 새로운 프로세스를 도입해야 하는가 여부를 결정하시오.
        -   (A)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=3))$
        -   (B)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=3))$

---

-   R코드로 판단 기준 정보 확인 진행

```{r}
n <- 30
pa <- 0.2
pb <- 0.1

# 1. 30건 중에서 미준수가 1건도 없을 확률
c(pbinom(0, n, pa), pbinom(0, n, pb))

# 2. 30건 중에서 미준수가 6건 이상 발생할 확률
c(1-pbinom(5, n, pa), 1-pbinom(5, n, pb))

# 3. 미준수 건수가 4건 이상일 확률이 50%를 넘지 않는다면 현재 프로세스를 유지
c(1-pbinom(3, n, pa), 1-pbinom(3, n, pb))

```

-   정보: A가 B보다 약 2배 높다.
    -   1: B가 A보다 약 42배 정도 높다.
    -   2: A가 B보다 약 8배 정도 높다.
    -   3: A가 0.877, B가 0.353로 A에 새로운 process 도입이 필요해 보인다.

---

## 실습 4. 포아송 분포를 이용한 확률 계산

-   카메라의 인식 성능을 조사하기 위하여 78종류의 대상들에 대하여 반복 인식 시험을 실시하였다. 이는 주어진 사진 또는 사물을 촬영한 후 고유의 알고리즘을 통하여 인식된 대상을 구별한다. 각 대상을 올바로 분류하지 못하는 경우를 결함으로 정의하였다. 자동화 Test인 관계로 정확한 시행 횟수는 측정하지 않았으며, 인식 대상에 따라 약 3500 ~ 3600번 정도 진행된다. 실제 제품에서는 대상이 모호한 경우, 재측정을 2회 진행한다. 각 대상에서 결함이 2회 이하로 발생활 확류을 구하고, 어떤 대상들의 결함이 심각한지를 확인하시오.

---

-   데이터 불러오기

```{r}
test <- read.csv(paste0(data_path, "recognition.test.csv"))
str(test)
```

-   defect1 ~ defect30로 30개의 결함에서 해당 대상들에 대해 해당 결함이 몇 번 발생했는지 파악할 수 있다.

---

-   결함율 계산 및 시각화

```{r}
test <- test %>% 
  mutate(id = fct_reorder(id, mean),
         success_p = ppois(2, mean))
str(test)
```

-   각 대상의 결함들의 mean값과, 조건(**[각 대상에서 결함이 2회 이하로 발생]{style="color: red;"}**)을 만족하는 확률을 구했다.

```{r}
test %>% ggplot(aes(x = fct_reorder(id, -mean),
                    y = mean)) +
  geom_bar(stat = "identity",
           color = "white", fill = lre) +
  labs(x = "인식 대상", y = "결점 평균") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 5))
```

-   먼저, 결점 평균 그래프를 보면, 몇몇 대상에서는 평균 결점이 6번 이상으로 높게 나타나는 것을 볼 수 있다.
-   이런 대상들은 카메라의 인식 성능이 낮아 개선이 필요해 보인다.
-   다음으로는 각 대상에서 결함이 2회 이하로 발생할 확률에 대한 그래프를 살펴 본다.

```{r}
test %>% ggplot(aes(x = fct_reorder(id, success_p),
                    y = success_p)) +
  geom_bar(stat = "identity",
           color = "white", fill = sbl) +
  geom_hline(yintercept = 0.05, color = lre) +
  geom_hline(yintercept = 0.10, color = lre) +
  labs(x = "인식 대상", y = "조건 만족 확률") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 5))
```

-   각 대상에서 결함이 2회 이하로 발생한 확률을 가지고 그래프를 그려보았다.
-   해당 그래프를 보면 5% 기준하에서는 몇몇 대상만이 조건을 만족하는 것을 알 수 있다.
-   10% 기준하에서는 절반이상의 대상들이 조건을 만족하는 것을 알 수 있다.
-   각각의 기준하에서 어떤 대상들이 인식 성능이 낮은지 파악하는게 필요해보이고, 원인을 파악해야 할 것으로 보인다.

```{r}
over_10_percent_ids <- test %>% 
  filter(success_p > 0.10) %>% 
  pull(id)

print(over_10_percent_ids)
```
-   파악한 결과, f196 f34  t100 m80  t136 p114 f134 t170 p131 h31  m48  m92  h200 t139 f83  p2   f32  m70  p120 t103 f14  p164 h91 으로 나타났다.
-   추가적으로 해당 대상들에서 어떤 결함이 많이 발생하는지 파악을 해보았다.

```{r}
# 10% 이상의 success_p 값을 가지는 ID들을 필터링
high_defect_ids <- test[test$success_p > 0.10,]

# 10% 이상의 success_p 값을 가지는 데이터의 각 결함의 평균 값을 계산
avg_defects <- colMeans(high_defect_ids[,c('defect1', 'defect2', 'defect3', 'defect4', 'defect5', 'defect6', 'defect7', 'defect8', 'defect9', 'defect10', 'defect11', 'defect12', 'defect13', 'defect14', 'defect15', 'defect16', 'defect17', 'defect18', 'defect19', 'defect20', 'defect21', 'defect22', 'defect23', 'defect24', 'defect25', 'defect26', 'defect27', 'defect28', 'defect29', 'defect30')]) %>% 
  sort(decreasing = TRUE)

print(avg_defects)
# 가장 높은 결함 값 파악
```

-   많이 발생한 결함과 대상을 가지고 품질을 위해 다시 노력을 하면 좋은 결과를 볼 수 있을 것이다.

---

## 실습 5. 서로 다른 난이도의 비교

-   김열공 학생은 00자격 시험에 2번 응시하였다. 1차 응시에서는 270/320점을 획득하였고, 2차 시험에서는 290/320점을 획득하였다. 원하는 300/320점을 획득하지는 못했지만 시험 성적이 높아지고 있으니 곧 300점을 넘을 수 있을 것이라고 생각하고 있다. 1차 시험을 치른 전체 학생들의 점수는 $N(250, 10^2)$을 따르며 2차 시험을 치른 전체 학생들의 점수는 $N(270, 15^2)$을 따른다. 김철수 학생은 실제로 어떤 시험을 더 잘 보았는가?

---

-   의사 결정 기준값 계산

```{r}
mu1 <- 250
mu2 <- 270
sigma1 <- 10
sigma2 <- 15
c(pnorm(270, mu1, sigma1), pnorm(290, mu2, sigma2))
```

-   각각 270점, 290점 이하의 점수를 얻을 확률을 의미를 한다.

```{r}
c(1-pnorm(270, mu1, sigma1), 1-pnorm(290, mu2, sigma2))
```

-   방금 계산한 값에서 1을 뺌으로써 해당 점수보다 높은 점수를 얻을 확률을 구한다.

---

-   결과의 시각화

```{r}
prob1 <- 1-pnorm(270, mu1, sigma1)
prob2 <- 1-pnorm(290, mu2, sigma2)
#
x <- seq(200, 320, length.out = 1000)
y1 <- dnorm(x, mu1, sigma1)
y2 <- dnorm(x, mu2, sigma2)
df <- data.frame(x, y1, y2)
#
ggplot(df, aes(x, y1)) +
  geom_line(color = lre) +
  geom_ribbon(data = subset(df, x > 270),
              aes(ymax = y1), ymin = 0, fill = lre,
              alpha = 0.5) +
  geom_line(aes(x, y2), color = sbl) +
  geom_ribbon(data = subset(df, x > 290),
              aes(ymax = y2), ymin = 0, fill = sbl,
              alpha = 0.5) +
  geom_hline(yintercept = 0) +
  labs(title = "1/2차 시험 결과", x = "시험 점수",
       y = "확률 밀도") +
  theme_bw() +
  annotate("text", x = 276, y = 0.003,
           label = round(prob1, 3)) +
  annotate("text", x = 300, y = 0.006,
           label = round(prob2, 3))
```

-   1차 시험은 0.0228, 2차 시험은 0.0912이다. 
-   성적의 퍼진 정도를 파악을 해보면 1차 시험은 붉은색 곡선이 더 뾰족한 것으로 보아, 학생들의 성적이 더 밀집되어 있는 것을 알 수 있다.
-   2차 시험은 파란색 곡선이 빨간색 곡선보다 완만한 것으로 보아, 성적이 더 넓게 퍼져있다는 걸 알 수 있다.
-   2차 시험에서 점수 자체는 높았지만 난이도가 1차 시험에 비해 쉬웠다고 볼 수 있다.
    -   아니면 대체로 많은 학생들의 1차 시험에 비해 공부를 많이하여 시험 성적이 이전보다 향샹했다고 볼 수도 있다.
-   따라서 1차 시험에서는 상위 0.0228(약 2%), 2차 시험에서는 상위 0.0912(약 9%)로 김철수 학생은 실제로 1차 시험에서 더 잘 봤다고 할 수 있다.


---

## 실습 6. 서로 다른 단위의 실행력 비교

-   A 업무는 처리 시간(분)이 중요 특성이며 이는 $N(30, 3^2)d$을 따른다. 또한 만일 처리 시간이 35분을 넘는 경우 고객 불만족이 발생하는 것으로 알려져 있다.
-   B 업무는 불순물의 정제량(g)이 중요 특성이며 이는 $N(200, 25^2)$을 따른다. 또한 정제량이 150g보다 적은 경우 고객 불만족이 발생하는 것으로 알려져 있다.
-   A 업무와 B 업무 중에서 어느 업무가 고객 불만족을 유발 할 가능성이 높은가?

---

-   의사 결정 기준값 계산

```{r}
mu1 <- 30
mu2 <- 200
sigma1 <- 3
sigma2 <- 25
c(1-pnorm(35, mu1, sigma1), pnorm(150, mu2, sigma2))
```

-   고객 불만족 기준에 따라서 고객 불만족 가능성을 계산을 해보았다.
-   A 업무의 고객 불만족 확률: 0.0478 (즉, 4.78%)
-   B 업무의 고객 불만족 확률: 0.0228 (즉, 2.28%)
    -   A 업무에서 처리 시간이 35분을 넘는 경우에 고객 불만족이 발생할 확률은 4.78%이다.
    -   반면 B 업무에서 정제량이 150g보다 적을 때 고객 불만족이 발생할 확률은 2.28%으로 나타났다.
    -   A업무의 고객 불만족 확률이 B 업무보다 높다고 할 수 있다.
    -   따라서 A 업무가 고객 불만족을 유발 할 가능성이 더 높다. 


---

# 3. 확증적 비교 방법 Confirmatory Comparisons

-   프로세스란 제품/서비스 결과물의 모임이 아니라 요소와 절차의 모임이다.
-   주어진 목적과 데이터의 유형에 따라 적절한 분석 방법을 선택해야 한다.
-   통계적 비교 방법의 유용성은 **[통계적 가정을 만족]{style="color: red;"}**한다는 전제 하에서 보장되는 것이다.

| | Continuous Data | Categorical Data |
|:--------------------:|:---------------------------:|:------------------------------:|
|       1 집단         |          1-sample t         |           1-proportion         |
|       2 집단         |          2-sample t         |            2-proportions       |
|      2 집단 쌍체     |           Paired t          |           McNemar's Test       |
|     2 집단 이상      |           ANOVA             |           Chi-square Test      |

-   범주형 데이터
    -   집계/측정 단위 선택
    -   범주 결정 빈도 집계
-   -> 이산형 Discrete Data

## 3.1. 범주형/이산형 데이터를 이용한 비교

-   MECE
    -   Mutually&Exclusive: Distinctly Different 겹치지 말고
    -   Collectively&Exhaustive: All Together, Make a Complete Whole 빠트리지 말고
    -   쉽게 생각하면 짜장면, 짬뽕, 국수??.... -> MECE 실패

### 3.1.1. 모비율의 비교1

-   단일 모비율
    -   **[stats::prop.test(x, n, p, alternative = c("less", "greater", "two sided"))]{style="color: blue;"}**
    
-   두 집단의 모비율 비교
    -   **[stats::prop.test(x, y, n, p, alternative = c("less", "greater", "two.sided"))]{style="color: blue;"}**

---

### 3.1.2. 모비율의 비교2

-   교차표 Contingency Table
    -   2개 이상의 변수들의 빈도 분포를 표현하는 행렬 형식의 표
    -   이항, 포아송, 다항 분포를 따르는 이산형 데이터의 요약에 유용

-   쌍체 집단의 모비율 비교 McNemar’s Test

| Group/Results | Go | NoGo | Total |
|---|----|------|------|
| Go | p11 | p12 | p1+ |
| NoGo | p21 | p22 | p2+ |
| Total | p+1 | p+2 | p++ |

|Group/Results| Go | NoGo | Total |
|---|----|------|------|
| Go | n11 | n12 | n1+ |
| NoGo | n21 | n22 | n2+ |
| Total | n+1 | n+2 | n++ |

$H_0 : p1+ - p+1 = 0$ vs $H_1 : p1+ - p+1 ≠ 0$

주어진 유의수준 알파에서 다음을 만족하면 $H_0$을 기각한다.  

$χ^2_M = \frac{(|n12 - n21| - 1)^2}{n12 + n21}$
$χ^2_M > χ^2_{(1,α)}$ 또는 $$ p-value < α $$

---

-   독립성/동질성 검정 Independent Test

**H₀**: Independence $p_{ij} = p_{i+}p_{+j}$ vs. $H₁$: not $H₀$

-   카이 제곱 통계량

$$\chi^2 = \sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \sim \chi^2_{(r-1)(c-1)}$$

여기서 $E_{ij} = \frac{n_{i+}n_{+j}}{n_{++}}$

-   Pearson's Residuals

$$r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}, \forall i,j$$


---

## 3.2. 연속형 데이터를 이용한 비교

### 3.2.1. 정규성 검정 Normality Test

### 3.2.2. 단일 집단 모평균 비교 1-sample t Test

### 3.2.3. 상호 독립인 두 집단 모평균 비교 2-sample t Test

### 3.2.4. 쌍체 집단 차이의 모평균 비교 paired-t Test

### 3.2.5. 두 개 이상의 모평균 비교 Analysis of Variance

---

## 실습 7. 범주형 데이터의 Wrangling/Munging : C-T-F customer.sat.csv

-   범주형 데이터를 가지고 handling 하면서 다루는 방법을 이해한다.
-   Case to Table/Frequency
    -   **[stats::xtabs(formula, data)]{style="color: blue;"}**: 교차표 작성
    -   **[as.data.frame(table object)]{style="color: blue;"}**: 변수 중심 전환

-   Table to Case/Frequency
    -   **[tidyr::uncount(dataframe, weights)]{style="color: blue;"}**
    -   **[as.data.frame(table object)]{style="color: blue;"}**

-   Frequency to Table/Case
    -   **[stats::xtabs(Freq ~ variables, data)]{style="color: blue;"}**
    -   **[tidyr::uncount(dataframe, weights)]{style="color: blue;"}**


---

-   데이터 불러오기

```{r}
customer <- read.csv(paste0(data_path, "customer.sat.csv"))
```

-   데이터 파악

```{r}
str(customer)
head(customer, 5)
```

-   individual to table

```{r}
cas_to_tab <- xtabs(~ item + group, customer)
head(cas_to_tab, 5)
```

-   individual to count

```{r}
temp <- xtabs(~ item + group, customer)
cas_to_freq <- as.data.frame(temp)
head(cas_to_freq, 5)
```

-   table to individuals

```{r}
temp <- as.data.frame(cas_to_tab)
tab_to_cas <- tidyr::uncount(temp, weights = Freq)
head(tab_to_cas, 5)
```

-   table to count

```{r}
tab_to_freq <- as.data.frame(cas_to_tab)
head(tab_to_freq, 5)
```

-   count to table

```{r}
freq_to_tab <- xtabs(Freq ~ item + group,
                     tab_to_freq)
head(freq_to_tab, 5)
```

-   count to individuals

```{r}
freq_to_cas <- tidyr::uncount(
                cas_to_freq,
                weights = Freq)
head(freq_to_cas, 5)
```



---

## 실습 8. 모비율의 검정

-   다음은 자동화 생산의 핵심 공정인 [E단계]와 [R 단계]에서 조사된 불량의 개수이다. 조사 과정에서는 핵심 Para.s 또는 재료의 변경을 제한하였고, 이전 조사에 의하면 두 공정의 불량률은 거의 동일하다.
    -   1. 각 단계의 불량률이 0.07보다 작다고 할 수 있는지를 검정하시오.
    -   2. 현재 조사된 데이터를 이용하여 고려된 공정의 불량률이 동일한지를 확인하시오.

---

|             | E 단계 | R 단계 |
|:-----------:|:-------:|:-------:|
| Defect      |   72    |   28    |
| Non-Defect  |  928    |  672    |
| Total       | 1000    |  700    |

---

-   E단계

```{r}
prop.test(72, 1000, 0.07, alternative = "less")
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.6 > $alpha$=0.05 이므로 $H_{0}$ 기각 못한다.
-   E단계의 프로세스는 최대 8.7%의 불량률이 나타날 수 있다.

---

-   R단계

```{r}
prop.test(28, 658, 0.07, alternative = "less")
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.004 < $alpha$=0.05 이므로 $H_{0}$ 기각한다.
-   R단계의 프로세스는 최대 5.8%의 불량률이 나타날 수 있다.

---

-   두 단계의 불량률 비교

```{r}
defect <- c(72, 28)
n <- c(1000, 700)
prop.test(defect, n)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.008 < $alpha$=0.05 이므로 $H_{0}$ 기각한다.
    -   신뢰구간 또한 0값을 포함하고 있지 않다.
-   두 프로세스의 불량률은 동일하다고 볼 수 없다.
-   최소 0.9%, 최대 5.5의 차이를 보일 수 있다.
-   따라서 E단계의 공정률은 조정이 필요해 보인다.

---

## 실습 9. 모비율의 검정2 customer.ds.csv

-   다음은 특정 제품을 구매한 지 1년이 지난 고객 중에서 n=1000명을 선택하여 전화 조사를 통하여 디자인(design)과 서비스(service) 만족도를 5점 척도로 조사한 것이다.
-   (1) 4점 이상을 "만족"으로 가정한다
-   (2) 고려한 제품은 디자인과 서비스 중에서 어떤 특성이 더 만족도가 높은가?

-   데이터 불러오기

```{r}
ds <- read.csv(paste0(data_path, "customer.ds.csv"))
str(ds)
```

```{r}
# 막대그래프 생성 (dodge position)
ds1_plot <- ggplot(data = ds, aes(x = design, fill = factor(service), group = service)) +
  geom_bar(position = "dodge") +
  labs(x = "Design", y = "Freq", fill = "Service") +
  theme_bw()

# 막대그래프 생성 (fill position)
ds2_plot <- ggplot(data = ds, aes(x = design, fill = factor(service), group = service)) +
  geom_bar(position = "fill") +
  labs(x = "Design", y = "Percent", fill = "Service") +
  scale_y_continuous(labels = scales::percent) +
  theme_bw()

gridExtra::grid.arrange(ds1_plot, ds2_plot, ncol = 2)
```

-   만족 고객 구분 및 비교

```{r}
ds$d.satisfied <- ifelse(ds$design >= 4, "Yes", "No")
ds$s.satisfied <- ifelse(ds$service >= 4, "Yes", "No")
comparison <- table(ds$d.satisfied, ds$s.satisfied)

prop.test(comparison)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.7 > $\alpha$이므로 $H_{0}$기각 하지 못한다.
    -   따라서 두 특성의 만족도는 서로 다르다고 할 수 없다.

---

## 실습 10. 쌍체 데이터의 비율 비교

-   광고의 유용성을 확인하기 위하여 잠재 구매 고객을 두 개의 Group으로 나누고 광고를 보기 전에 제품을 구매할 것인가의 여부(buy, not buy) 그리고 동일한 고객이 광고를 접하고 난 후의 제품 구매 여부를 조사하였다. 이는 서로 다른 고객간의 의견 변동을 최소화 하기 위하여 설계되었다.
-   광고의 유용성 여부를 확인하시오.

| Before \ After | buy  | not buy | Total |
|----------------|------|---------|-------|
| buy            | 734  | 66      | 800   |
| not buy        | 96   | 704     | 800   |
| **Total**      | 830  | 770     | 1600  |

---


-   데이터 입력

```{r}
consumer <- matrix(c(734, 96, 66, 704), nrow = 2, ncol = 2)
dimnames(consumer) <- list(before.adv = c("buy", "not buy"),
                           after.adv = c("buy", "not buy"))
consumer
```

-   검정 진행하기

```{r}
mcnemar.test(consumer)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.02 < $\alpha$이므로 $H_{0}$기각한다.
    -   따라서 광고 전/후의 고객 구매 응답률은 서로 동일하지 않다.

---

## 실습 11. 지역/모델 별 고객 Claim 집계 claim.area.2019.csv

-   다음은 일정 기간 동안 조사된 제품(model) 별 Customer/Partner 불만 사항의 빈도를 지역 별로 집계한 것이다.

---

-   데이터 불러오기

```{r}
claim19 <- read.csv(paste0(data_path, "claim.area.2019.csv"))
str(claim19)
```


-   데이터 구조 확인

```{r}
my_color <- c(lre, sbl, ngr, lgo)
#
g1 <- claim19 %>% 
        ggplot(aes(x = model, y = frequency, fill = area)) +
          geom_bar(stat = "identity", position = "dodge") +
          labs(title = "Contingency Table",
               x = "모델명", y = "빈도") +
          scale_fill_manual(values = my_color) +
          theme_bw()
#
g2 <-   claim19 %>% 
          ggplot(aes(x = model, y = frequency, fill = area)) +
            geom_bar(stat = "identity", position = "fill") +
            labs(title = "Contingency Table",
                 x = "모델명", y = "빈도") +
            scale_fill_manual(values = my_color) +
            scale_y_continuous(labels = scales::percent) +
            theme_bw()
gridExtra::grid.arrange(g1, g2, ncol =2)
```


-   Extra Visualiation1

```{r}
claim19_t <- xtabs(frequency ~ model + area, claim19)
vcd::doubledecker(claim19_t)
```

-   카이제곱 독립성 검정
    -   모든 셀의 기대 도수(Expected)는 5보다 커야 한다. 그렇지 않으면 경고 메시지를 출력한다.
    
```{r}
result <- chisq.test(claim19_t)
result
```
```{r}
result$p.value
result$expected
```

-   Extra visualization2

```{r}
claim19_t <- xtabs(frequency ~ model + area, claim19)
vcd::mosaic(claim19_t,
            gp = shading_max,
            labeling = labeling_residuals,
            digits = 2
            )
```

---

-   데이터 부분 선택: 해외향

```{r}
claim19_2 <- claim19 %>% 
  dplyr::filter(area != "1Domestic")
claim19_t2 <- xtabs(frequency ~ model + area, claim19_2)
vcd::doubledecker(claim19_t2, spacing = spacing_highlighting,
                  main = "Customer Claims 2019")
```

```{r}
result_2 <- chisq.test(claim19_t2)
result_2
```
```{r}
result_2$p.value
result_2$expected
```

```{r}
mosaic(claim19_t2, gp = shading_max, labeling = labeling_residuals, digits = 2)
```




---

## 실습 12. 정규성 검정 Normality Test mid.measure.csv

-   다음은 생산 프로세으의 중간 단계에서 수집된 데이터로 목표값은 19.0(Micrometer, 0.001mm)이다.

---

-   데이터 불러오기 및 구조 확인
```{r}
check <- read.csv(paste0(data_path, "mid.measure.csv"))
str(check)
```

-   목표값 달성 유무와 산포 확인

```{r}
mean(check$length)
sd(check$length)
```


```{r}
summary(check)
```

-   분포 확인

```{r}
check %>% 
  ggplot(aes(length, y = ..density..)) +
  geom_histogram(color = "white", fill = "gray") +
  geom_density(color = lre) +
  geom_density(color = sbl, adjust = 2) +
  geom_density(color = ngr, adjust = 0.5) +
  theme_bw()
```

-   Probability(Q-Q) Plot

```{r}
check %>% 
  ggplot(aes(sample = length)) +
  stat_qq(color = sbl) +
  stat_qq_line(color = lre) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 1, linetype = "dashed") +
  labs(title = "Probability(Q-Q) plot",
       x = "Theoretical quantiles(Standard Normal)",
       y = "Sample quantiles(Data)"
       ) +
  theme_bw()
```

-   정규성 검정

```{r}
ad.test(check$length)
pearson.test(check$length)
ks.test(check$length, "pnorm")
```

-   그룹 별 정규성 검정

```{r}
ad_tests <- lapply(split(check$length, check$set), ad.test)
p_values <- sapply(ad_tests, function(x) x$p.value)
p_values

check %>% 
  ggplot(aes(sample = length, color = set)) +
  geom_qq() +
  stat_qq_line(aes(color = set)) +
  labs(title = "Q-Q plot by sets",
       x = "Standard Normal", y = "Data") +
  theme_bw()
```

-   그룹 별 모수(들) 추정
    -   단일 그룹 변수의 범주 별로 모수(그리고 표준오차)를 추정

```{r}
mid <- pars_group(check, length, set, "normal")

mid %>% 
  bind_rows() %>% 
  mutate(mean = purrr::map_dbl(fit, "mean"),
         sd = purrr::map_dbl(fit, "sd")
         ) %>% 
  select(-fit)
```

-   what should we do now?

```{r}
check %>% 
  ggplot(aes(x = length, color = set)) +
  geom_density(adjust = 2) +
  theme_bw()
```

---

## 실습 13. 정규성 변환(Box Cox Transformation) auto.defect.csv

-   다음은 반복 자동화 테스트에서 최초 결점이 발생할 때 까지의 시간(sec.s)을 조사한 것이다.

---

-   데이터 불러오기 및 구조 확인

```{r}
defect <- read.csv(paste0(data_path, "auto.defect.csv"))
str(defect)
```

-   평균과 산포 확인

```{r}
mean(defect$times)
sd(defect$times)
```

-   정규정 검정

```{r}
ad.test(defect$times)
pearson.test(defect$times)
ks.test(defect$times, "pnorm")
```

-   Box-Cox Transformation

```{r}
x <- defect$times
bc <- boxcox(x ~ 1)
lambda <- round(bc$x[which.max(bc$y)], 2)
x_trans <- (x^lambda -1) / lambda
ad.test(x_trans)
```

-   변환 후 분포 확인

```{r}
defect$trans0.2 <- x_trans
```

-   변환 전 후 mean 값 비교하기
```{r}
mean <- mean(defect$times)
mean_by_trans <- ((mean(x_trans)*lambda) + 1)^(1/lambda)
c(mean, mean_by_trans)
```


---

## 실습 14. 신제품 인증 시험 검증, touch.ms.csv



---

## 실습 15. 소음 개선 후 Pilot Sample 개선 검증, noise.db.csv

---

## 실습 16. 쌍체 데이터의 차이의 모평균 비교, exposure.con.csv

---

## 실습 17. 분산 분석, depth.etch.csv



---

# 4. 단순 회귀 분석 Simple Regression Analysis


## 4.1. 회귀 계수의 추정: 최소 제곱법(Method of Least Squares)

## 4.2. 잔차 residuals

## 4.3. 변동의 분해 decompostion of variations

## 4.4. 회귀 계수의 유의성 검정

## 실습 17.. simple.csv: 임의로 선택된 소형 화물의 무게$(x;~g)$과 부피$(y;~cm^3)$

-   예제1 simple.csv
-   무게에 의하여 운송 요금을 결정하는 소형 화물들 중에서 임의로 선택된 58개의 무게(x; weight)과 부피(y; volume)을 조사하였다.
-   이를 이용하여 중량과 부피간의 관계를 추정하여 운송 계획을 수립하고자 한다.

```{r}
cargo <- read.csv(paste0(data_path, "simple.csv"))
str(cargo)

na_count <- function(df){
  sapply(df, function(y) sum(length(which(is.na(y)))))
}

na_count(cargo)
x <- cargo$x
y <- cargo$y
Sxy <- sum((x - mean(x)) * (y - mean(y)))
Sxx <- sum((x - mean(x))^2)
Syy <- sum((y - mean(y))^2)
#
hat_beta1 <- Sxy / Sxx
hat_beta0 <- mean(y) - hat_beta1*mean(x)
round(c(hat_beta0, hat_beta1), 2)


```

---

## 실습 18. 유통 매장 정리 시간, dist.store.csv

-   데이터 불러오기

```{r}
dist <- read.csv(paste0(data_path, "dist.store.csv"))
```

-   데이터 확인

```{r}
str(dist)
head(dist)
tail(dist)
view(dist)
```

```{r}
dist <- dist %>% 
  rename_all(tolower)

head(dist, 5)
```
```{r}
na_count <- function(df){
  sapply(df, function(y) sum(length(which(is.na(y)))))
}

na_count(dist)
```

---

-   시각화: 관계의 검토

```{r}
dist %>% 
  ggplot(aes(case, time)) +
  geom_point(color = sbl) +
  theme_bw()
```

-   (case, time)의 관계는 직선의 경향을 갖는다.

---

-   회귀 분석의 실행

```{r}
dist_fit <- lm(time ~ case, data = dist)
```

-   분석 결과 확인

```{r}
summary(dist_fit)
```


---

## 실습 19. Test Case Determination, test.sw.csv


```{r}

```


---


# 5. 다중 회귀 분석 Multiple Regression Analysis

## 5.1. 최소 제곱법과 중요 결과

---

## 5.2. 변수 선택 Variable Selection

### 5.2.1. 전방 선택 Foreward Selection

### 5.2.2. 후방 제거 Backward Elimination

### 5.2.3. 단계별 선택 Stepwise Selection

### 5.2.4. 모든 부분 집합 

---

## 실습 20. C-materials, cem.materials.csv

-    데이터 불러오기

```{r}
cem <- read.csv(paste0(data_path,"cem.materials.csv"))
str(cem)
```

-   변수 이름과 위치 정리

```{r}
cem <- cem %>% 
  dplyr::rename_all(tolower) %>% 
  select(-id) %>% 
  relocate(y)
```

-   결측값 확인

```{r}
na_count <- function(data){
  sapply(data, function(y) sum(length(which(is.na(y)))))
}
```

```{r}
na_count(cem)
```

-   데이터 시각화: 행렬도(matrix plot)

```{r}
cem %>% 
  ggpairs(columns = 1:ncol(.),
          lower = list(continuous = wrap("points", # continuous = "smooth"
                                         color = sbl,
                                         aplha = 0.5)
                   ),
          upper = list(continuous = wrap("points", # wrap("cor", size = 2.5)
                                         color = sbl,
                                         alpha =0.5)
                   ),
          diag = list(continuous = wrap("densityDiag",
                                        color = "black",
                                        alpha = 0.5)
                   ),
          title = "Component Data") +
  theme_bw()
```

-   함수 정의
    -   원하는 형태의 행렬도(matrix plot) 형식을 결정하여 함수화
    
```{r}
gg_mplot <- function(df, ttl){
  df %>% 
  ggpairs(columns = 1:ncol(.),
          lower = list(continuous = wrap("points", #continuous = "smooth"
                                         color = sbl,
                                         alpha = 0.5)
                       ),
          upper = list(continuous = wrap("points", #wrap("cor", size = 2.5)
                                         color = sbl,
                                         alpha = 0.5)
                       ),
          diag = list(continuous = wrap("densityDiag",
                                        color = "black",
                                        alpha = 0.5)
                       ),
          title = ttl) +
  theme_bw()
}

ttl = "Component Data: function 활용"
gg_mplot(cem, ttl)
```


-   다중 회귀 분석: lm()
    -   lm() 함수는 단순 회귀 분석과 동일하게 적용된다.
    -   단, formula를 사전에 지정하는 경우에는 문자형으로 정의되어야 한다.

```{r}
fml <- "y ~ t1 + t2 + t3 + t4"
cem_model <- lm(fml, cem)
summary(cem_model)
```

-   구간 추정
    -   lm()에 의한 model object가 생성
    
```{r}
confint(cem_model, level = 0.95)
```

```{r}
confint(cem_model, c("t1", "t2"), level = 0.95)
```

-   가설 검정
    -   lm()을 적용한 model object의 summary()의 coef를 부분 선택
  
```{r}
summary(cem_model)$coef
```

-    예측 구간

```{r}

```
