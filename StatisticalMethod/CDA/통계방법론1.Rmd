---
title: "통계 방법론1 중간대체"
author: "응용통계학과 이준혁"
date: "2023-10-28(토)"
output:
  html_document:
    css: styles.css
    #code_folding: show
    fig_caption: yes
    fig_height: 7.5
    fig_width: 10
    fig_retina: null
    highlight: haddock
    self_contained: yes
    theme: cosmo
    toc: yes
    toc_depth: 6
    toc_float: yes
    fig_dpi: 300
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style type="text/css">
  body, td {
     font-size: 16px;
     font-family: 맑은 고딕
  }
  code.r{
    font-size: 16px;
    font-weight: bold;
    font-family: 맑은 고딕
  }
  pre {
    font-size: 14px
    font-family: 맑은 고딕
  }
  h1,h2,h3,h4,h5,h6{
    font-family: 맑은 고딕;
    font-weight: bold;
  }
  h1{
    font-size: 18pt;
  }
  h2{
    font-size: 16pt;
  }
  h3{
    font-size: 14pt;
  }
  table{
    font-size: 20px;
  }
</style>
<br><br><br><br>

---

# 목차

## 귀납적 추론은 새로운 지식이 세상에 들어오는 유일한 과정이다.($\epsilon$이 unknown일 때)
-   생각하기
-   Part2 Statistical Method - 요약, 비교, 관계
  -   1. 통계적 의사 결정 Statistical Decision Making
  -   2. 확률 분포와 그 응용
  -   3. 확증적 비교 방법 Confirmatory Comparisons
  -   4. 단순 회귀 분석 Simple Regression Analysis
  -   5. 다중 회귀 분석 Multiple Linear Regressionwwww
-   중요한 기본 R함수 정리
  
---

# 생각하기

-   자유도
-   통계적 의사 결정
    -   모든 관심 특성은 **[변수]{style="color: red;"}**이다.
    -   모든 통계량은 고유의 **[표본 분포(sampling distribution)]{style="color: red;"}**를 갖는다.
        -   이는 표본 추출에 의하여 발생하는 **[표본 오차(sampling error)]{style="color: red;"}**를 수량화한다.
    -   표본 오차(sampling error)가 존재한다면, 이를 **[의사결정에 반영]{style="color: red;"}**할 수 있어야 한다.
-   연속형일 때는 측정 단위가 매우 중요하다 -> 표준화 적용
-   다변량에서 차원 축소를 진행할 때 y값을 다 표준화하는 게 추세이다.
-   쌍체 검정은 쌍별 차이를 고려하기 때문에 개체 내의 변동성은 제거하고 측정값 간의 차이에만 집중을 할 수 있다.
    -   동일한 주체로부터 두 번의 측정을 받기 때문에, 주체의 개별적인 차이나 특성에 의한 영향을 제거할 수 있다.
-   개선 활동의 검증에는 일반적으로 **[방향성 가설]{style="color: red;"}**이 적절하다.
-   설명 변수들의 선형 족속 관계일 때 다중 공선성이 존재한다.
    -   왜 문제인가?
        -   $(X`X)^-1$이 불안정해지고, $var(\hat\beta)$도 불안해지고, $\hat y$도 불안정해진다.
    -   파생변수를 생성한뒤에는 활용한 기존 변수는 제거를 해야하는가?

-   ML 적용
    -   heuristics에 의해서 $\alpha=0.05$로 설정하는 것처럼 머신러닝에서는 heuristics에 의해 선택되는 것이 많이 있다.
    -   A/B test 방법 중에 하나가 T-test로 봐도 되는 것인가?
    -   기본Base 모델인 A모델과 B모델을 비교하고자한다.
    -   모델 선택도 heuristics에 의해서 좋은 모델을 선택한다.
        -   $$H_0 : \mu_B \leq \mu_A$$, B모델의 성능은 A모델의 성능보다 나쁘거나 같다.
        -   $$H_1 : \mu_B > \mu_A$$, B모델의 성능은 A모델의 성능보다 좋다.
        -   같은 샘플을 가지고 ML모델을 적용해서 T-test를 통해서 모델을 비교한다.(대응 T-test)
    -   Multiple Comparison: 단순 2모델 비교가 아닐 때는 Multiple Comparison
        -   본페로니 보정을 통해서 더 엄격한 $\alpha$값을 설정, $\alpha/모델 개수$

---

# 1. 통계적 의사 결정 Statistical Decision Making

## 1.1. 모수와 통계량

-   **모수:** 모집단의 특징으로 **[상수(constant)]{style="color: red;"}**
-   **통계량:** 표본에서의 결과이며 **[변수(variable)]{style="color: red;"}**

```{r}
unit1 = c(10, 10, 10, 50, 30, 40, 50, 40, 30 ,50)
unit2 = c(20, 30, 40, 10, 20, 20, 20, 30, 50, 40)

results <- ((unit1 + unit2) / 2) %>% print()

# 결과를 데이터 프레임으로 변환
df <- data.frame(Sample.Mean = results)
df

# 도트 플롯으로 시각화하기
ggplot(df, aes(x = Sample.Mean)) + 
  geom_dotplot(binwidth = 0.5, stackdir = "center", color=sbl, fill= sbl) +
  labs(title="모평균과 표본 평균(n = 2)", x="표본 평균", y="빈도") +
  ylim(0, 1) +
  theme_bw() + 
  geom_vline(xintercept=30, linetype="dashed", color = lre)
```

-   모든 통계량은 고유의 분포를 갖는다. 이를 **[표본 분포(sampling distribution) ]{style="color: red;"}**라고 한다.
-   모든 모집단의 평균이 의미가 있는가?
    -   평균만으로는 분포의 형태, 이상치의 영향, 데이터가 가지는 의미를 파악하기에는 어려움이 있어 보인다.
  
```{r}
# 1. Normal Population Distribution
set.seed(123)
population1 <- rnorm(1000, mean=100, sd=5)
sample_means1 <- replicate(1000, mean(sample(population1, size=30)))

p1 <- ggplot(data.frame(x=population1), aes(x)) + geom_density() + xlim(80, 120)
p2 <- ggplot(data.frame(x=sample_means1), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(97, 103)
gridExtra::grid.arrange(p1, p2, ncol = 2)
```
```{r}
# 2. Gamma Distribution
population2 <- rgamma(1000, shape=1, scale=100)
sample_means2 <- replicate(1000, mean(sample(population2, size=30)))

p3 <- ggplot(data.frame(x=population2), aes(x)) + geom_density() + xlim(0, 300)
p4 <- ggplot(data.frame(x=sample_means2), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(50, 150)
gridExtra::grid.arrange(p3, p4, ncol = 2)

```
```{r}
# 3. Exponential Distribution
population3 <- rexp(1000, rate=0.005)
sample_means3 <- replicate(1000, mean(sample(population3, size=30)))

p5 <- ggplot(data.frame(x=population3), aes(x)) + geom_density() + xlim(0, 1000)
p6 <- ggplot(data.frame(x=sample_means3), aes(x)) + geom_histogram(aes(y=..density..), bins=30) + xlim(0, 600)
gridExtra::grid.arrange(p5, p6, ncol = 2)
```

###  대수의 법칙 Law of Large Numbers

```{r}
# 데이터 생성
n <- 10000

# Normal(100, 10)
normal_1 <- rnorm(n, mean=100, sd=10)
cum_normal_1 <- cumsum(normal_1) / (1:n)

# Normal(90, 20)
normal_2 <- rnorm(n, mean=90, sd=20)
cum_normal_2 <- cumsum(normal_2) / (1:n)

# Uniform(40, 120)
uniform_data <- runif(n, min=40, max=120)
cum_uniform <- cumsum(uniform_data) / (1:n)

# 그래프 그리기
plot_df <- data.frame(
  Iteration = 1:n,
  Normal_100_10 = cum_normal_1,
  Normal_90_20 = cum_normal_2,
  Uniform_40_120 = cum_uniform
)

ggplot(plot_df, aes(Iteration)) +
  geom_line(aes(y = Normal_100_10, color = "Normal(100, 10)")) +
  geom_line(aes(y = Normal_90_20, color = "Normal(90, 20)")) +
  geom_line(aes(y = Uniform_40_120, color = "Uniform(40, 120)")) +
  labs(y = "표본 평균", x = "표본 크기", title = "[LLM] 분포의 형태와 산포 그리고 표본 크기에 따른 표본 평균 값의 변화") +
  theme_bw() +
  scale_color_manual(values = c(sbl, lre, lgo))
```


-   산포가 작고 정규분포일 때 수렴이 더 빠른 것을 알 수 있다.
-   $n_0<n_1$이어도 항상 $n_0$에서의 요약값보다 참값에 근접하지는 않다.

### 중심 극한 정리 Central Limit Theorem; CLT

-   **[모집단 분포와 관계없이,]{style="color: red;"}** 표본 크기(n)가 증가함에 따라 표본 평균의 분포는 정규 분포로 근사한다.
-   표본 크기(n)가 커질수록 표본 평균의 분산이 작아진다.

    -   표준 오차(Standard Error; SE)
        -   통계량의 **[표준 편차]{style="color: red;"}**
        -   표본 오차를 수량화한 것으로 **[그 값이 작을수록 바람직]{style="color: red;"}**
$$SE(\bar{y}) = \frac{\sigma}{\sqrt{n}}$$



### 중요 모수와 통계량

|  | 모수 | 통계량값 | 정의범위 |
|:------:|:----:|:-------:|:--------:|
| 비율 - 이산형 | $p$ | $\hat{p} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = 0,1$ |
| 평균 간격 - 이산형 | $\lambda$ | $\hat{\lambda} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = 0,1,2,...$ |
| 평균 - 연속형 | $\mu$ | $\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i$ | $y_i = [-\infty, \infty]$ |
| 분산 | $\sigma^2$ | $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2$ | $\forall y_i$ |

$$\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i = \frac{1}{n}y_1 + \frac{1}{n}y_2 + \dots + \frac{1}{n}y_n$$

-   **[표본 평균의 평균]{style="color: red;"}**은 모평균이다. 
    -   $E(\bar{y}) = \mu$
-   정보 자체는 연속형이 더 많이 내포하고 있기 때문에 어떻게 하면 이산형 변수를 근사화할 수 있지에 대한 고민이 중요하다.
-   예를 들어 사람의 키가 170.5cm라는 값이 있는데 이를 '키가 크다'라고 분류해버리면 이는 정보의 손실에 해당한다.

## 1.2. 중요 통계량의 표본 분포

-   표본 분포는 (1)표본 크기(n)의 영향을 받고 (2)통계량의 표본 오차를 표현한다.

### 카이 제곱 분포 $\chi^2$

-   카이 제곱분포
    -   표준정규분포 제곱합(Sum of Squared)의 분포
    -   $Z_i \sim N(0,1)$, i = 1, 2, ..., v가 상호 독립일 때
    -   $C = \sum_{i=1}^{v} Z_i^2 \sim \chi^2(v)$, $v$는 자유도
    -   $E(C) = v$
    -   $\text{Var}(C) = 2v$
    -   평균이 커지면 분산이 커지는 분포이다.
    
* 두 표본이 서로 독립이라면, $P(X=x, Y=y) = P(X=x)P(Y=y)$

-   카이제곱 분포의 **[가법성]{style="color: red;"}**
    -   가법성 개념이 중요하다.
        -   계산이 편리해진다. -> 해석이 용이해진다.
        
$$C_1 \sim \chi^2(v_1),~C_2 \sim \chi^2(v_2)~\text{상호 독립이라면,}$$

$$C_1 + C_2 \sim \chi^2(v_1 + v_2)$$

-   실무적으로 상호 독립을 설명하기가 어렵다.
    -   랜덤하게 추출되었으면 상호독립일 가능성이 높다.
    -   **[교차표]{style="color: red;"}**를 정리해서 검증하면 좋다.

```{r}
# x 축을 위한 데이터 벡터 생성 (0부터 80까지)
x <- seq(0, 80, by = 0.1)

# 자유도가 4, 15, 29, 39, 49인 카이제곱 분포의 확률밀도함수 계산
y_df1 <- dchisq(x, df = 4)
y_df2 <- dchisq(x, df = 14)
y_df3 <- dchisq(x, df = 29)
y_df5 <- dchisq(x, df = 39)
y_df10 <- dchisq(x, df = 49)

# 데이터 프레임 생성
df <- data.frame(x, y_df1, y_df2, y_df3, y_df5, y_df10)

# 그래프 그리기
ggplot(df, aes(x = x)) +
  geom_line(aes(y = y_df1, color = 'df = 4')) +
  geom_line(aes(y = y_df2, color = 'df = 14')) +
  geom_line(aes(y = y_df3, color = 'df = 29')) +
  geom_line(aes(y = y_df5, color = 'df = 39')) +
  geom_line(aes(y = y_df10, color = 'df = 49')) +
  ggtitle("자유도에 따른 카이제곱분포의 형태") +
  xlab("x") +
  ylab("Density") +
  scale_color_manual(values = c(lre, sbl, ngr, 'purple', lgo)) +
  theme_bw()
```

-   고려 사항
    -   언제 사용하는가?
        -   (1)모분산의 통계적 추론
        -   (2)범주형 변수의 **[독립성/동일성 검정]{style="color: red;"}**
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code
    
```{r}
qchisq(0.025, df = 49) #31.6
qchisq(0.975, df = 49) #70.2
```

---

### t분포 $t_{\nu}$

- 독립변수 $Z$ 와 $C$ 가 각각 $N(0,1)$, $C \sim \chi^2(v)$ 인 경우를 가정하면,

$$ T = \frac{Z}{\sqrt{C/v}}~\sim t(v) $$

- $E(T) = 0$
- $Var(T) = \frac{v}{v-2}, v > 2$

---

-   Why t ?

$$ \bar{y} \sim N \left( \mu, \frac{\sigma^2}{n} \right) $$

$$ Z = \frac{\bar{y} - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1) $$

$$ \frac{(n-1)s^2}{\sigma^2} \sim  \chi^2(n-1) $$

따라서,

$$ T = \frac{Z}{\sqrt{\frac{(n-1)s^2}{\sigma^2(n-1)}}} = \frac{\bar{y} - \mu}{\sqrt{s^2/n}} \approx t(n-1) $$


-   t-분포와 정규분포 비교

```{r}
# x 범위 정의
x <- seq(-5, 5, by=0.01)

# 표준 정규 분포
pdf_norm <- dnorm(x, mean=0, sd=1)

# t-분포 (자유도 3, 10, 25)
pdf_t3 <- dt(x, df=3)
pdf_t10 <- dt(x, df=10)
pdf_t25 <- dt(x, df=25)

# 데이터프레임 생성
df <- data.frame(x, pdf_norm, pdf_t10, pdf_t3, pdf_t25)

# 그래프 그리기
ggplot(df, aes(x=x)) +
  geom_line(aes(y=pdf_norm, color='Normal Distribution')) +
  geom_line(aes(y=pdf_t10, color='t-Distribution (df=10)'), linetype="dashed") +
  geom_line(aes(y=pdf_t3, color='t-Distribution (df=3)'), linetype="dotted") +
  geom_line(aes(y=pdf_t25, color='t-Distribution (df=25)'), linetype="twodash") +
  labs(title="Standard Normal vs. t-Distributions",
       x="Value",
       y="Density",
       color="Distribution") +
  theme_bw()
```

-   오른쪽과 왼쪽 꼬리가 두껍다. 정규분포에 비해 변동성 증가한다.
    -   why? t분포는 작은 표본 크기에서의 표본 평균의 분포를 설명하기 때문에 이러한 특성을 가진다.
-   고려 사항
    -   언제 사용하는가?
        -   모분산을 모르는 경우, 모평균에 대한 통계적 추론
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code
    
```{r}
qt(0.025, df = 10) # 결과: -2.23
qt(0.025, df = 30) # 결과: -2.04
qnorm(0.025) # 결과: -1.96
qt(0.975, df = 10) # 결과: 2.23
qt(0.975, df = 30) # 결과: 2.04
qnorm(0.975) # 결과: 1.96
```

-   이를 통해서 n이 커짐에 따라서 정규 분포로 근사하는걸 알 수 있다.

---

### F분포 $F_{\nu1, \nu2}$

F-분포는 두 개의 자유도 $\nu_1$과 $\nu_2$를 가진 카이제곱 분포의 비율에 대한 분포로 정의된다.

$F = \frac{C/\nu_1}{U/\nu_2}$


$E(F) = \frac{\nu_2}{\nu_2 - 2}, \quad \nu_2 > 2$

$Var(F) = \frac{2\nu_2(\nu_1+\nu_1)}{\nu_1(\nu_2-2)^2(\nu_2-4)}, \quad \nu_2 > 4$

-   why F?

$$(n_1 - 1) \frac{s_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$$

$$(n_2 - 1) \frac{s_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$$

$$F = \frac{\frac{(n_1-1)s_1^2}{\sigma_1^2} / (n_1 - 1)}{\frac{(n_2-1)s_2^2}{\sigma_2^2} / (n_2 - 1)} = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2} \sim F(n_1-1, n_2-1)$$




```{r}
# x 범위 정의
x <- seq(0, 5, by=0.01)

# F 분포 자유도 조합 정의
dfs <- list(c(10,10), c(10,30), c(10,5), c(30,10), c(30,30), c(30,5), c(5,10), c(5,30), c(5,5))

# 각 자유도 조합에 대한 F 분포 계산
pdfs <- lapply(dfs, function(df) df(x, df1=df[1], df2=df[2]))

# 데이터프레임 생성
df_data <- data.frame(x = rep(x, length(dfs)),
                      y = do.call(c, pdfs),
                      df = factor(rep(sapply(dfs, paste, collapse="-"), each=length(x))))

# 그래프 그리기
ggplot(df_data, aes(x=x, y=y, color=df)) +
  geom_line() +
  labs(title="F distribution for various degrees of freedom",
       x="Value",
       y="Density",
       color="Degrees of Freedom") +
  theme_bw()
```

-   고려 사항
    -   언제 사용하는가?
        -   (1)모분산의 비에 대한 통계적 추론
        -   (2)분산 분석
    -   분포의 모수는 무엇인가?
        -   표본 크기(n)에 의하여 결정되는 **[자유도]{style="color: red;"}**
    -   확률 계산하는 R Code

```{r}
qf(0.025, df1 = 5, df2 = 30) #0.161
qf(0.975, df1 = 5, df2 = 30) #3.03
```

---

## 1.3. 통계적 추론 Statistical Inference

-   표본 오차가 존재한다면, 이를 의사 결정 과정에 반영할 수 있어야 한다.
-   실무적 고려 사항은 통계적 고려 사항의 전제 조건이다.

### 1.3.1. 구간 추정 Interval Estimation

-   주어진 신뢰도를 만족하는 신뢰구간을 설정하여 미지의 모수 또는 모수들의 함수를 추정하는 방법
    -   95% 신뢰구간을 선택하는 것은 heuristics이다.
    -   표본의 크기가 늘어나면 불확실성이 줄어드는 것이지 참값에 도달한다는 것은 아니다.
        -   **[표본을 적절하게 설정하는 것이 중요!]{style="color: red;"}**
    
$~~~~~~~~(1-\alpha)100\% \text{ 신뢰구간}$

-   모수(도는 모수의 함수) $\theta$와 이에 대한 추정량 $\hat{\theta}$에 대하여 다음을 만족하는 구간 $[L,~U]$

$~~~~~~~~\Pr(L \leq \theta \leq U) = 1 - \alpha, \ 0 < \alpha < 1$

-   실제 모수는 구해진 신뢰 구간 내에 존재한다.
-   가능하면, 최악의 경우를 고려하여, 신뢰 하한(Lower Limit) 또는 신뢰 상한(Upper Limit) 중의 하나로 해석한다.

-   단일 모평균의 구간 추정
$$
T = \frac{\bar{y} - \mu}{s/\sqrt{n}} \sim t(n-1), s^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (y_i - \bar{y})^2
$$

-   $(1-\alpha)100\% \text{ 신뢰 구간}$

$$
\text{Pr} \left[ |T| \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$

$$
\text{Pr} \left[ -t_{\alpha/2}(n - 1) \leq T \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$
$$
\text{Pr} \left[ -t_{\alpha/2}(n - 1) \leq \frac{\bar{y} - \mu}{s/\sqrt{n}} \leq t_{\alpha/2}(n - 1) \right] = 1-\alpha \\
$$
$$
\text{Pr} \left[ \bar{y} - t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \leq \mu \leq \bar{y} + t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \right] = 1-\alpha \\
$$
따라서
$$
[L, U] = \left[ \bar{y} - t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}}, \bar{y} + t_{\alpha/2}(n - 1) \frac{s}{\sqrt{n}} \right]
$$

---

### 1.3.2. 가설 검정

-   증명하고자 하는 주장을 대립 가설로 설정한다.
-   (모집단의 특성에 대한) 어떤 주장이나 추측을 가설로 설정하고 표본 관찬을 통하여 **[이의 기각(rejection) 여부]{style="color: red;"}**를 결정하는 방법
-   통계적 가설
    -   귀무가설 Null Hypothesis: $H_{0}$
        -   기존에 알려진 사실을 구체적으로 표현한 가설
        -   반드시 "=" 포함한다.
    -   대립가설 Alternative Hypothesis: $H_{1}$
        -   분석자가 Data를 통하여 사실임을 입증하고자 하는 가설
-   가설의 종류 양측 vs 단측
    -   성능 향상을 보려는 목적이면 단측이 적합하다.
-   의사결정 오류 Errors of Decision Making
    -   제 1종의 오류$(\alpha)$
        -   $H_{0}$가 사실일 때 $H_{0}$를 기각하는 오류
    -   제 2종의 오류$(\beta)$
        -   $H_{0}$가 거짓일 때 $H_{0}$를 기각하지 못하는 오류
-   유의수준 Significant level
    -   제 1종의 오류를 범할 확률 허용한계
    -   전통적으로 0.05를 많이 가정한다.
-   p-값 p-value
    -   $H_{0}$하에서 $H_{0}$를 기각할 최소의 유의 수준
-   의사결정
    -   p-value < $\alpha$이면, $H_{0}$를 기각
    
---

## 실습 1. 구간 추정 결과의 해석

-   제품간/프로세스간/업무실적간 개선 전/후의 모평균을 비교하고자 한다.
    -   단, 반응값(y)는 큰 값이 바람직하다.
        -   1) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (-30.5, -10.5)이다.
            -   (1) 0이 미포함되었으므로 개선이 되었다.
            -   (2) 음수가 되었다는 건 $\mu2$으로 개선이 되었다.($\mu2$평균이 더 높다다)
            -   (3) 최소 -10.5 개선되었다. -> **["최악의 경우"]{style="color: red;"}**로 봐야 한다.
                -   -30.5가 개선되었다고 하면 **[과대해석; 과장광고의 느낌]{style="color: #FAB23D;"}**이다.
        -   2) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (-10.3, 10.7)이다.
            -   (1) 0이 포함되었으므로 개선이 되지 않았다고 해석 진행
        -   3) 모평균의 차 $\mu1-\mu2$의 95% 신뢰구간이 (10.7, 30.2)이다.
            -   (1) 0이 미포함되었으므로 개선이 되었다.
            -   (2) 양수가 되었다는 건 $\mu1$으로 개선이 되었다.
            -   (3) 최소 10.7 개선되었다. -> **["최악의 경우"]{style="color: red;"}**로 봐야 한다.
        -   따라서 신뢰 구간의 하한을 기준으로 해석하는 것이 합리적인 접근 방식이다.
        
---

## 실습 2. 통계적 추론의 응용, storage.lot.csv

-   TU 에너지 저장 장치는 개별 LoT에서 자동 생산된다. 각 LOT에서는 자동 선별 및 측정을 통하여 일정 수를 대상으로 저장 능력을 검증하고 있으며, 검증 횟수는 LOT별로 일정하지 않다.
-   중요 관리 기준은 각 LOT별 제품의 에너지 저장 능력의 평균이며, 만일 LOT의 평균 생산 능력이 985.0보다 작다면, 해당 LOT는 재점 대상이 된다.  

---

-   데이터 불러오기
```{r}
perform <- read.csv(paste0(data_path, "storage.lot.csv"))
str(perform)
```

-   데이터 시각화

```{r}
limit <- 985

perform %>% 
  ggplot(aes(x = lot, y = storage, group = lot)) +
  geom_boxplot(position = position_dodge(width = 0.8), color = sbl) +
    labs(x = "lot_no", y = "storage") +
    geom_hline(yintercept = limit, color = lre) +
    scale_x_continuous(breaks=seq(1, 100, 1)) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    theme(axis.text.x = element_text(size = 6))
```

-  lot별로 요약이 필요해보인다.
    -   재점검 판단이 불가능하다.

---

-   데이터 요약1

```{r}
p_sum <- perform %>% 
  group_by(lot) %>% 
  summarize(lmean = mean(storage), lsd = sd(storage))

head(p_sum, 3)
```

```{r}
p_sum %>% ggplot(aes(lmean, lsd)) +
  geom_point(color = sbl) +
  geom_vline(xintercept = 985, color = lre, linetype = "dashed") +
  theme_bw()
```

-   이렇게 표현을 하게 되면 대부분의 lot가 985.0보다 작아 재점검 대상이 된다.
    -   의미가 없는 시각화이다.
    -   따라서 각 "lot"의 평균값의 신뢰구간을 표현하여 다시 분석을 실시한다.  

-   데이터 요약2: 95% 신뢰구간

```{r}
perform_c <- perform %>%
  group_by(lot) %>%
  summarize(
    avg = mean(storage),
    lower = mean(storage) - qt(0.975, n() - 1) * (sd(storage) / sqrt(n())),
    upper = mean(storage) + qt(0.975, n() - 1) * (sd(storage) / sqrt(n()))
  ) %>%
  ungroup()

perform_c <- as.data.frame(perform_c)

perform_c %>%
  ggplot(aes(x = lot, y = avg)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper, color = ifelse(upper < 985, "A", "B")), width = 0.2) +
  scale_color_manual(values = c("A" = lre, "B" = sbl)) +
  guides(color = FALSE) +
  labs(x = "lot", y = "Average") +
  theme_bw()
```

-   대부분의 "lot"는 평균값 주변에 데이터가 밀집되어 있어 변동성이 크지 않아 보인다.
-   빨간색 바로 표시되는 "lot"는 상한값이 985.0보다 작은 것을 의미한다.
-   따라서3개의 "lot"가 관리 기준을 만족하지 못하는 것으로 파악이 된다.
    -   재점검 실시


---

# 2. 확률 분포와 그 응용

-   확률 이론은 계산으로 축소된 상식일 뿐이다.
-   다음 세가지를 고민하면서 접근해야 어떤 문제를 맞닿았을 때 고민을 해보고 적용할 수 있다.
    -   언제 사용하는가?
        -   해당 확률 분포나 방법이 적절한 상황을 판단하는데 중요하다.
    -   확률 분포를 결정하는 모수(들)은 무엇인가?
    -   관심 사상에 대한 확률을 어떻게 계산할 것인가?

---

-   실무적 관심 특성에 대한 중요 확률 분포

| 분포 유형 | 분포 이름     | 모수          | R 함수   | 기호                |
|----------|--------------|--------------|---------|-------------------|
| 이산형   | 이항 Binomial | n, p         | `binom()` | $Bin(n, p)$       |
|          | 포아송 Poisson| $\lambda$   | `pois()`  | $Poi(\lambda)$    |
| 연속형   | 정규 Normal   | $\mu, \sigma^2$ | `norm()` | $N(\mu, \sigma^2)$ |

---

-   R에서 쓰는 확률 분포 함수들 의미:
$$
d*: \text{확률 밀도(질량) 함수 probability mass(density) function } p(\theta) = f(y; \theta) \\
p*: \text{누적 분포 함수 cumulative distribution function } F(y) = Pr(Y \leq y) \\
q*: \text{역 누적 분포 함수 inverse cumulative distribution function } F^{-1}(p) = \inf\{z : F(z) \geq p\} \\
r*: \text{랜덤 숫자 생성 함수 random number generation function } r_1, r_2, ... r_n \sim iid \ p(\theta) \\
$$

---

## 2.1. 이산형 확률 분포

-   유한 또는 가변 값을 취할 수 있는 확률변수의 확률 분포
-   일정 단위에서 특정 범주의 **[빈도]{style="color: blue;"}**를 수집하는 경우 유용하다.
-   개별 비율 vs 누적 비율
    -   누적 비율로 봐야 한다. -> **[비율은 누적]{style="color: red;"}**한다.

---

### 2.1.1. 이항 분포 Binomial Distribution

-   $Y$ ~ $Bin(n,p)$

-   조건 및 특성:
    -   베르누이 시행을 $n > 1$ 번 독립적으로 시행했을 때 특정 사건의 수
    -   특정 모비율 $p$을 갖는 (무한)모집단으로부터 추출한 단위 표본(n) 중 관심 사건의 수
    -   n 번의 시행에서 매회 성공률이 p, $0 < p < 1$로 동일함을 가정

-   확률(질량) 함수:

$$
f(y) = \binom{n}{y} p^y (1-p)^{n-y}, y = 0,1,2,..., n
$$

-   평균: $E(Y) = np$

-   분산: $Var(Y) = np(1 - p)$

-   분산은 평균의 함수이다.
---

```{r}
# Binomial 분포의 데이터 생성
x <- 0:20
y1 <- dbinom(x, 20, 0.1) # Bin(20, 0.1)의 pdf
y2 <- dbinom(x, 20, 0.5) # Bin(20, 0.5)의 pdf

y1_cdf <- pbinom(x, 20, 0.1) # Bin(20, 0.1)의 CDF
y2_cdf <- pbinom(x, 20, 0.5) # Bin(20, 0.5)의 CDF

data_pdf <- data.frame(x, y1, y2)
data_cdf <- data.frame(x, y1_cdf, y2_cdf)

# pdf plot
pdf_plot <- ggplot(data_pdf, aes(x = x)) +
  geom_line(aes(y = y1, color = "Bin(20, 0.1)"), size = 1) +
  geom_point(aes(y = y1, color = "Bin(20, 0.1)")) +
  geom_line(aes(y = y2, color = "Bin(20, 0.5)"), size = 1) +
  geom_point(aes(y = y2, color = "Bin(20, 0.5)")) +
  labs(title = "pdf of Binomial Distributions", y = "확률", x = "x") +
  theme_minimal()

# cdf plot
cdf_plot <- ggplot(data_cdf, aes(x = x)) +
  geom_step(aes(y = y1_cdf, color = "Bin(20, 0.1)"), size = 1) +
  geom_point(aes(y = y1_cdf, color = "Bin(20, 0.1)")) +
  geom_step(aes(y = y2_cdf, color = "Bin(20, 0.5)"), size = 1) +
  geom_point(aes(y = y2_cdf, color = "Bin(20, 0.5)")) +
  labs(title = "CDF of Binomial Distributions", y = "누적 확률", x = "x") +
  theme_minimal()

#
gridExtra::grid.arrange(pdf_plot, cdf_plot, ncol = 2)
```

-   Bin(20, 0.1) vs Bin(20, 0.5)
    -   빨간색 선은 성공확률의 0.1인 20번의 시도에 대한 분포이다. 0~5사이에 집중되어 있다.
        -   따라서 20번의 시도중에서 0~5번 성공할 확률이 높다는 것을 알 수 있다.
    -   파란색 선은 성공확률의 0.5인 20번의 시도에 대한 분포이다. 10 근처에서 가장 높을 값을 가진다.
        -   즉, 20번의 시도 끝에 10번 성공할 확률이 가장 높음을 의미한다.
    -   누적 확률을 확인해봐도 앞서 봤던 결과처럼 빨간색선은 x = 5일때 누적확률이 거의 1에 가까워지는 것을 알 수 있다.

---

### 2.1.2. 포아송 분포 Poisson Distribution

-   $Y$ ~ $Poi(\lambda)$

-   확률 (질량)함수:
$$f(y) = \frac{\lambda^y e^{-\lambda}}{y!}$$
여기서, $y = 0, 1, 2, ...$

-   기대값: $E(Y) = \lambda$

-   분산: $Var(Y) = \lambda$

-   $Y_i \sim Poi(\lambda_i), i = 1, 2, ..., k$

$$\sum_{i=1}^{k} Y_i \sim Poi(\sum_{i=1}^{k} \lambda_i)$$

-    분산은 평균의 함수이다.

```{r}
# 포아송 분포의 데이터 생성
x <- 0:20
y1 <- dpois(x, 3) # Poi(3)의 pdf
y2 <- dpois(x, 10) # Poi(10)의 pdf

y1_cdf <- ppois(x, 3) # Poi(3)의 CDF
y2_cdf <- ppois(x, 10) # Poi(10)의 CDF

data_pdf <- data.frame(x, y1, y2)
data_cdf <- data.frame(x, y1_cdf, y2_cdf)

# pdf plot
pdf_plot <- ggplot(data_pdf, aes(x = x)) +
  geom_line(aes(y = y1, color = "Poi(3)"), size = 1) +
  geom_point(aes(y = y1, color = "Poi(3)")) +
  geom_line(aes(y = y2, color = "Poi(10)"), size = 1) +
  geom_point(aes(y = y2, color = "Poi(10)")) +
  labs(title = "pdf of Poisson Distributions", y = "확률", x = "x") +
  theme_minimal()

# cdf plot
cdf_plot <- ggplot(data_cdf, aes(x = x)) +
  geom_step(aes(y = y1_cdf, color = "Poi(3)"), size = 1) +
  geom_point(aes(y = y1_cdf, color = "Poi(3)")) +
  geom_step(aes(y = y2_cdf, color = "Poi(10)"), size = 1) +
  geom_point(aes(y = y2_cdf, color = "Poi(10)")) +
  labs(title = "CDF of Poisson Distributions", y = "누적 확률", x = "x") +
  theme_minimal()

#
gridExtra::grid.arrange(pdf_plot, cdf_plot, ncol = 2)
```

-   각각 사건의수가 3일때와 10일 때의 부근에서 많이 일어난 것을 알 수 있다.

```{r}

```


## 2.2. 연속형 확률 분포

-   연속형 확률 변수
    -   임의의 구간 또는 구간들의 합을 취할 수 있는 확률 변수
    -   구간/비율 척도로 측정되며, 유무형의 측정 장비에 의하여 확인이 가능
    -   측정 장비를 적용하는 경우, 고유의 측정 단위 존재

---

-   연속형 확률 분포를 결정하는 요소
    -   1. 기술적: 관심 특성의 기술적 특징
    -   2. 환경적: 데이터의 측정 환경
    -   3. 상식적: 실무 현상에 대한 객관성과 타당성
    
---

-   연속형 확률 분포는 **[형태(shape), 위치(location), 척도(scale)]{style="color: red;"}**에 의하여 결정

---

-   연속형 확률밀도 함수
    -   1) 모든 가능한 y값에 대하여 확률 밀도 값은 0보다 크거나 같다.
    -   2) 모든 가능한 y값에서 구한 면적은 1이다.
    -   3) 관심 사상이 정의된 구간의 면적으로 해당 확률을 계산한다.

---

### 2.2.1. 정규분포 Normal Distributions

-   $Y$ ~ $N(\mu,\sigma^2)$



---

## 실습 3. 이항 분포를 이용한 확률 계산

-   지난 분기 개발 업무(A)d의 일정 미준수율은 약 20%정도이고 검증 업무 (B)의 시간 미준수율은 약 10%정도이다. 차기 분기 이전에 각각의 업무에서 임의로 선택된 30건에 대한 미준수 건수를 조사하여 개선 프로세스 도입 여부를 판단하고자 한다. (A)와 (B) 각 업무에 대하여 다음을 계산하시오.
    -   1. 30건 중에서 미준수가 1건도 없을 확률
        -   (A)의 경우: $P(X=0) = \binom{30}{0} (0.2)^0 (0.8)^{30}$
        -   (B)의 경우: $P(X=0) = \binom{30}{0} (0.1)^0 (0.9)^{30}$
    -   2. 30건 중에서 미준수가 6건 이상 발생할 확률
        -   (A)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=5))$
        -   (B)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=5))$
    -   3. 미준수 건수가 4건 이상일 확률이 50%를 넘지 않는다면 현재 프로세스를 유지하고, 만일 50%를 넘는다면 새로운 관리 기준을 도입하려 한다. 두 업무는 새로운 프로세스를 도입해야 하는가 여부를 결정하시오.
        -   (A)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=3))$
        -   (B)의 경우: $1-(P(X=0)+P(X=1), ..., +P(X=3))$

---

-   R코드로 판단 기준 정보 확인 진행

```{r}
n <- 30
pa <- 0.2
pb <- 0.1

# 1. 30건 중에서 미준수가 1건도 없을 확률
c(pbinom(0, n, pa), pbinom(0, n, pb))

# 2. 30건 중에서 미준수가 6건 이상 발생할 확률
c(1-pbinom(5, n, pa), 1-pbinom(5, n, pb))

# 3. 미준수 건수가 4건 이상일 확률이 50%를 넘지 않는다면 현재 프로세스를 유지
c(1-pbinom(3, n, pa), 1-pbinom(3, n, pb))

```

-   정보: A가 B보다 약 2배 높다.
    -   1: B가 A보다 약 42배 정도 높다.
    -   2: A가 B보다 약 8배 정도 높다.
    -   3: A가 0.877, B가 0.353로 A에 새로운 process 도입이 필요해 보인다.

---

## 실습 4. 포아송 분포를 이용한 확률 계산, recognition.test.csv

-   카메라의 인식 성능을 조사하기 위하여 78종류의 대상들에 대하여 반복 인식 시험을 실시하였다. 이는 주어진 사진 또는 사물을 촬영한 후 고유의 알고리즘을 통하여 인식된 대상을 구별한다. 각 대상을 올바로 분류하지 못하는 경우를 결함으로 정의하였다. 자동화 Test인 관계로 정확한 시행 횟수는 측정하지 않았으며, 인식 대상에 따라 약 3500 ~ 3600번 정도 진행된다. 실제 제품에서는 대상이 모호한 경우, 재측정을 2회 진행한다. 각 대상에서 결함이 2회 이하로 발생활 확류을 구하고, 어떤 대상들의 결함이 심각한지를 확인하시오.

---

-   데이터 불러오기

```{r}
test <- read.csv(paste0(data_path, "recognition.test.csv"))
str(test)
```

-   defect1 ~ defect30로 30개의 결함에서 해당 대상들에 대해 해당 결함이 몇 번 발생했는지 파악할 수 있다.

---

-   결함율 계산 및 시각화

```{r}
test <- test %>% 
  mutate(id = fct_reorder(id, mean),
         success_p = ppois(2, mean))
str(test)
```

-   각 대상의 결함들의 mean값과, 조건(**[각 대상에서 결함이 2회 이하로 발생]{style="color: red;"}**)을 만족하는 확률을 구했다.

```{r}
test %>% ggplot(aes(x = fct_reorder(id, -mean),
                    y = mean)) +
  geom_bar(stat = "identity",
           color = "white", fill = lre) +
  labs(x = "인식 대상", y = "결점 평균") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 5))
```

-   먼저, 결점 평균 그래프를 보면, 몇몇 대상에서는 평균 결점이 6번 이상으로 높게 나타나는 것을 볼 수 있다.
-   이런 대상들은 카메라의 인식 성능이 낮아 개선이 필요해 보인다.
-   다음으로는 각 대상에서 결함이 2회 이하로 발생할 확률에 대한 그래프를 살펴 본다.

```{r}
test %>% ggplot(aes(x = fct_reorder(id, success_p),
                    y = success_p)) +
  geom_bar(stat = "identity",
           color = "white", fill = sbl) +
  geom_hline(yintercept = 0.05, color = lre) +
  geom_hline(yintercept = 0.10, color = lre) +
  labs(x = "인식 대상", y = "조건 만족 확률") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.y = element_text(size = 5))
```

-   각 대상에서 결함이 2회 이하로 발생한 확률을 가지고 그래프를 그려보았다.
-   해당 그래프를 보면 5% 기준하에서는 몇몇 대상만이 조건을 만족하는 것을 알 수 있다.
-   10% 기준하에서는 절반이상의 대상들이 조건을 만족하는 것을 알 수 있다.
-   각각의 기준하에서 어떤 대상들이 인식 성능이 낮은지 파악하는게 필요해보이고, 원인을 파악해야 할 것으로 보인다.

```{r}
over_10_percent_ids <- test %>% 
  filter(success_p > 0.10) %>% 
  pull(id)

print(over_10_percent_ids)
```
-   파악한 결과, f196 f34  t100 m80  t136 p114 f134 t170 p131 h31  m48  m92  h200 t139 f83  p2   f32  m70  p120 t103 f14  p164 h91 으로 나타났다.
-   추가적으로 해당 대상들에서 어떤 결함이 많이 발생하는지 파악을 해보았다.

```{r}
# 10% 이상의 success_p 값을 가지는 ID들을 필터링
high_defect_ids <- test[test$success_p > 0.10,]

# 10% 이상의 success_p 값을 가지는 데이터의 각 결함의 평균 값을 계산
avg_defects <- colMeans(high_defect_ids[,c('defect1', 'defect2', 'defect3', 'defect4', 'defect5', 'defect6', 'defect7', 'defect8', 'defect9', 'defect10', 'defect11', 'defect12', 'defect13', 'defect14', 'defect15', 'defect16', 'defect17', 'defect18', 'defect19', 'defect20', 'defect21', 'defect22', 'defect23', 'defect24', 'defect25', 'defect26', 'defect27', 'defect28', 'defect29', 'defect30')]) %>% 
  sort(decreasing = TRUE)

print(avg_defects)
# 가장 높은 결함 값 파악
```

-   많이 발생한 결함과 대상을 가지고 품질을 위해 다시 노력을 하면 좋은 결과를 볼 수 있을 것이다.

---

## 실습 5. 서로 다른 난이도의 비교

-   김열공 학생은 00자격 시험에 2번 응시하였다. 1차 응시에서는 270/320점을 획득하였고, 2차 시험에서는 290/320점을 획득하였다. 원하는 300/320점을 획득하지는 못했지만 시험 성적이 높아지고 있으니 곧 300점을 넘을 수 있을 것이라고 생각하고 있다. 1차 시험을 치른 전체 학생들의 점수는 $N(250, 10^2)$을 따르며 2차 시험을 치른 전체 학생들의 점수는 $N(270, 15^2)$을 따른다. 김철수 학생은 실제로 어떤 시험을 더 잘 보았는가?

---

-   의사 결정 기준값 계산

```{r}
mu1 <- 250
mu2 <- 270
sigma1 <- 10
sigma2 <- 15
c(pnorm(270, mu1, sigma1), pnorm(290, mu2, sigma2))
```

-   각각 270점, 290점 이하의 점수를 얻을 확률을 의미를 한다.

```{r}
c(1-pnorm(270, mu1, sigma1), 1-pnorm(290, mu2, sigma2))
```

-   방금 계산한 값에서 1을 뺌으로써 해당 점수보다 높은 점수를 얻을 확률을 구한다.

---

-   결과의 시각화

```{r}
prob1 <- 1-pnorm(270, mu1, sigma1)
prob2 <- 1-pnorm(290, mu2, sigma2)
#
x <- seq(200, 320, length.out = 1000)
y1 <- dnorm(x, mu1, sigma1)
y2 <- dnorm(x, mu2, sigma2)
df <- data.frame(x, y1, y2)
#
ggplot(df, aes(x, y1)) +
  geom_line(color = lre) +
  geom_ribbon(data = subset(df, x > 270),
              aes(ymax = y1), ymin = 0, fill = lre,
              alpha = 0.5) +
  geom_line(aes(x, y2), color = sbl) +
  geom_ribbon(data = subset(df, x > 290),
              aes(ymax = y2), ymin = 0, fill = sbl,
              alpha = 0.5) +
  geom_hline(yintercept = 0) +
  labs(title = "1/2차 시험 결과", x = "시험 점수",
       y = "확률 밀도") +
  theme_bw() +
  annotate("text", x = 276, y = 0.003,
           label = round(prob1, 3)) +
  annotate("text", x = 300, y = 0.006,
           label = round(prob2, 3))
```

-   1차 시험은 0.0228, 2차 시험은 0.0912이다. 
-   성적의 퍼진 정도를 파악을 해보면 1차 시험은 붉은색 곡선이 더 뾰족한 것으로 보아, 학생들의 성적이 더 밀집되어 있는 것을 알 수 있다.
-   2차 시험은 파란색 곡선이 빨간색 곡선보다 완만한 것으로 보아, 성적이 더 넓게 퍼져있다는 걸 알 수 있다.
-   2차 시험에서 점수 자체는 높았지만 난이도가 1차 시험에 비해 쉬웠다고 볼 수 있다.
    -   아니면 대체로 많은 학생들의 1차 시험에 비해 공부를 많이하여 시험 성적이 이전보다 향샹했다고 볼 수도 있다.
-   따라서 1차 시험에서는 상위 0.0228(약 2%), 2차 시험에서는 상위 0.0912(약 9%)로 김철수 학생은 실제로 1차 시험에서 더 잘 봤다고 할 수 있다.


---

## 실습 6. 서로 다른 단위의 실행력 비교

-   A 업무는 처리 시간(분)이 중요 특성이며 이는 $N(30, 3^2)d$을 따른다. 또한 만일 처리 시간이 35분을 넘는 경우 고객 불만족이 발생하는 것으로 알려져 있다.
-   B 업무는 불순물의 정제량(g)이 중요 특성이며 이는 $N(200, 25^2)$을 따른다. 또한 정제량이 150g보다 적은 경우 고객 불만족이 발생하는 것으로 알려져 있다.
-   A 업무와 B 업무 중에서 어느 업무가 고객 불만족을 유발 할 가능성이 높은가?

---

-   의사 결정 기준값 계산

```{r}
mu1 <- 30
mu2 <- 200
sigma1 <- 3
sigma2 <- 25
c(1-pnorm(35, mu1, sigma1), pnorm(150, mu2, sigma2))
```

-   고객 불만족 기준에 따라서 고객 불만족 가능성을 계산을 해보았다.
-   A 업무의 고객 불만족 확률: 0.0478 (즉, 4.78%)
-   B 업무의 고객 불만족 확률: 0.0228 (즉, 2.28%)
    -   A 업무에서 처리 시간이 35분을 넘는 경우에 고객 불만족이 발생할 확률은 4.78%이다.
    -   반면 B 업무에서 정제량이 150g보다 적을 때 고객 불만족이 발생할 확률은 2.28%으로 나타났다.
    -   A업무의 고객 불만족 확률이 B 업무보다 높다고 할 수 있다.
    -   따라서 A 업무가 고객 불만족을 유발 할 가능성이 더 높다. 


---

# 3. 확증적 비교 방법 Confirmatory Comparisons

-   프로세스란 제품/서비스 결과물의 모임이 아니라 요소와 절차의 모임이다.
-   주어진 목적과 데이터의 유형에 따라 적절한 분석 방법을 선택해야 한다.
-   통계적 비교 방법의 유용성은 **[통계적 가정을 만족]{style="color: red;"}**한다는 전제 하에서 보장되는 것이다.

| | Continuous Data | Categorical Data |
|:--------------------:|:---------------------------:|:------------------------------:|
|       1 집단         |          1-sample t         |           1-proportion         |
|       2 집단         |          2-sample t         |            2-proportions       |
|      2 집단 쌍체     |           Paired t          |           McNemar's Test       |
|     2 집단 이상      |           ANOVA             |           Chi-square Test      |

-   범주형 데이터
    -   집계/측정 단위 선택
    -   범주 결정 빈도 집계
-   -> 이산형 Discrete Data

## 3.1. 범주형/이산형 데이터를 이용한 비교

-   MECE
    -   Mutually&Exclusive: Distinctly Different 겹치지 말고
    -   Collectively&Exhaustive: All Together, Make a Complete Whole 빠트리지 말고
    -   쉽게 생각하면 짜장면, 짬뽕, 국수??.... -> MECE 실패

### 3.1.1. 모비율의 비교1

-   단일 모비율
    -   **[stats::prop.test(x, n, p, alternative = c("less", "greater", "two sided"))]{style="color: blue;"}**
    
-   두 집단의 모비율 비교
    -   **[stats::prop.test(x, y, n, p, alternative = c("less", "greater", "two.sided"))]{style="color: blue;"}**

---

### 3.1.2. 모비율의 비교2

-   교차표 Contingency Table
    -   2개 이상의 변수들의 빈도 분포를 표현하는 행렬 형식의 표
    -   이항, 포아송, 다항 분포를 따르는 이산형 데이터의 요약에 유용

-   쌍체 집단의 모비율 비교 McNemar’s Test

| Group/Results | Go | NoGo | Total |
|---|----|------|------|
| Go | p11 | p12 | p1+ |
| NoGo | p21 | p22 | p2+ |
| Total | p+1 | p+2 | p++ |

|Group/Results| Go | NoGo | Total |
|---|----|------|------|
| Go | n11 | n12 | n1+ |
| NoGo | n21 | n22 | n2+ |
| Total | n+1 | n+2 | n++ |

$H_0 : p1+ - p+1 = 0$ vs $H_1 : p1+ - p+1 ≠ 0$

주어진 유의수준 알파에서 다음을 만족하면 $H_0$을 기각한다.  

$χ^2_M = \frac{(|n12 - n21| - 1)^2}{n12 + n21}$
$χ^2_M > χ^2_{(1,α)}$ 또는 $$ p-value < α $$

---

-   독립성/동질성 검정 Independent Test

**H₀**: Independence $p_{ij} = p_{i+}p_{+j}$ vs. $H₁$: not $H₀$

-   카이 제곱 통계량

$$\chi^2 = \sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \sim \chi^2_{(r-1)(c-1)}$$

여기서 $E_{ij} = \frac{n_{i+}n_{+j}}{n_{++}}$

-   Pearson's Residuals

$$r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}, \forall i,j$$


---

## 3.2. 연속형 데이터를 이용한 비교

### 3.2.1. 정규성 검정 Normality Test

-   목적
    -   선택된 표본이 정규분포로부터 랜덤하게 선택되었는가 여부를 확인
-   가설
    -   $H_{0}$: 데이터는 정규분포를 따른다.
-   의사결정
    -   p-value < $\alpha$이면, $H_{0}$를 기각

---

-   정규성이 위배가 되면?
    -   평균이 올바른 대표값이 될 수 없다.
    -   정규성을 만족하는 경우와 비교할 때, 표준편차는 커지는 경향을 가진다.
-   **[표본 크기가 작은 경우, 정규성은 거의 위배되지 않는다.]{style="color: red;"}**
-   샘플 사이즈가 많다고 정규성이 위배되는 것은 아니다.

### 3.2.2. 단일 집단 모평균 비교 1-sample t Test

-   목적
    -   모집단의 평균$(\mu)$과 기준값$(\mu_{0})$의 차이 유무 및 대소 관계의 검증증
-   가설
    -   방향성 / 단측 가설
    -   비방향성 / 양측 가설
-   의사결정
    -   p-value < $\alpha$이면, $H_{0}$를 기각

---

-   기준값$(\mu_0)$은 명확한 근거 하에서 지정되어야 한다.
    -   ex) 고객의 요구 사항, 장기간 수집된 제품 특성값 평균
-   모집단의 정규성이 위배되는 경우, 검정 결과는 근사적 결과이다.

---

### 3.2.3. 상호 독립인 두 집단 모평균 비교 2-sample t Test

-   목적
    -   상호 독립인 두 집단의 모평균 비교

-   가설
    -   단측/방향설 가설
    -   비방향성/양측가설

-   의사 결정
    -   p-value < $\alpha$이면, $H_0$를 기각한다.
    
---

-   가능한 한 두 집단의 표본 크기는 동일하게 고려한다.
-   두 집단의 등분산성이 위배되는 경우에는 근사 검정을 적용한다.

---

### 3.2.4. 쌍체 집단 차이의 모평균 비교 paired-t Test

-   목적
    -   각 개체별로 반복 측정된 차이(difference)를 이용하여 두 집단 간 차이의 모평균 확인

-   가설
    -   단측/방향설 가설
    -   비방향성/양측가설

-   의사 결정
    -   p-value < $\alpha$이면, $H_0$를 기각한다.

---

-   데이터는 쌍별(pairwise)로 측정되어야 한다.
    -   따라서 두 집단의 표본 크기는 항상 동일하다.
-   각 개체/대상 별 차이를 계산하여 1-sample t-TEST를 한 결과와 동일하다.

---

-   쌍체 데이터와 상호 독립 데이터
    -   쌍체(Paried)
        -   처리 대상(sampling unit)간의 차이가 적으므로 보다 정확한 차이 검증이 가능
        -   일반적으로 양(+)의 상관 관계가 존재
            -   $Var(Y_{1}-Y_{2}) = Var(Y_{1}) + Var(Y_{2})-2Cov(Y_{1},Y_{2})$
    -   상호 독립(Independent)
        -   랜덤하게 선택된 서로 다른 대상에 처리 비교
        -   등분산성 확인 필요
        -   일반적으로 무(0)상관 가정
            -   $Var(Y_{1}-Y_{2}) = Var(Y_{1}) + Var(Y_{2})$

---

### 3.2.5. 두 개 이상의 모평균 비교 Analysis of Variance

-   목적
    -   변동(분산)을 기초로 2개 이상의 모평균을 비교하는 방법
    -   주로 3개 이상의 평균을 비교할 때 사용하는 2 sample t-test의 일반화된 방법

---

-   통계적 모형

$$y_{ij} = \mu_{i} + \varepsilon_{ij} = \mu + \tau_{i} + \varepsilon_{ij},~~i=1,2,...,a,~~j=1,2,...,r$$
$$\text{여기서, } \mu = (\sum_{i=1}^{a} \mu_{i})/a$$
$$\varepsilon_{ij} \sim iid \ N(0, \sigma^2)$$

-   통계적 가정 $\varepsilon_{ij}$ ~ $iid~~N(0, \sigma^2)$
    -   **독립성(Independence)**: 각 오차항은 상호 독립이다.
    -   **정규성(Normality)**: 오차는 정규분포를 따르며 모평균은 0이다.
    -   **등분산성(Equal Variances)**: 각 처리의 오차항은 동일한 분산 $\sigma^2$을 갖는다.

---

-   가설
    -   $H_{0} : \tau_{i} = 0, \ \forall i \ \text{vs.} \ H_{1} : \text{not } H_{0}$
    -   $\text{여기서, } \tau_{i} = \mu_{i} - \mu, \ i = 1, 2, ... , a~으로 효과(Effect)라고 부른다.$

-   의사 결정
    -   p-value < $\alpha$이면, $H_{0}$를 기각

---

-   ANOVA의 원리

    -   총변동($SS_{T}$) = 인자 변동($SS_{A}$) + 오차 변동($SS_{E}$)
    -   $H_0에서는~$$F_0$ ~ $F_{(a-1, n-a)}$

| 원인  | 제곱합 SS                  | 자유도 df  | 평균 제곱 MS   | $F_{0}$          |
|-------|--------------------------|-----------|--------------|--------------|
| $$인자$$  | $$SS_A = T \sum_{i=1}^{a} (y_i. - y..)^2$$ | $$a - 1$$     | $$MS_A$$     | $$F_0 = \frac{MS_A}{MS_E}$$ |
| $$오차$$  | $$SS_E = SS_T - SS_A$$   | $$n - a$$     | $$MS_E$$     |              |
| $$총합$$  | $$SS_T = \sum_{i=1}^{a} \sum_{j=1}^{n} (y_{ij} - y..)^2$$ | $$n - 1$$     |              |              |

![](C:\src\study\StatisticalMethod\CDA\png\분산분석의 원리.png)

---

## 실습 7. 범주형 데이터의 Wrangling/Munging : C-T-F, customer.sat.csv

-   범주형 데이터를 가지고 handling 하면서 다루는 방법을 이해한다.
-   Case to Table/Frequency
    -   **[stats::xtabs(formula, data)]{style="color: blue;"}**: 교차표 작성
    -   **[as.data.frame(table object)]{style="color: blue;"}**: 변수 중심 전환

-   Table to Case/Frequency
    -   **[tidyr::uncount(dataframe, weights)]{style="color: blue;"}**
    -   **[as.data.frame(table object)]{style="color: blue;"}**

-   Frequency to Table/Case
    -   **[stats::xtabs(Freq ~ variables, data)]{style="color: blue;"}**
    -   **[tidyr::uncount(dataframe, weights)]{style="color: blue;"}**


---

-   데이터 불러오기

```{r}
customer <- read.csv(paste0(data_path, "customer.sat.csv"))
```

-   데이터 파악

```{r}
str(customer)
head(customer, 5)
```

-   individual to table

```{r}
cas_to_tab <- xtabs(~ item + group, customer)
head(cas_to_tab, 5)
```

-   individual to count

```{r}
temp <- xtabs(~ item + group, customer)
cas_to_freq <- as.data.frame(temp)
head(cas_to_freq, 5)
```

-   table to individuals

```{r}
temp <- as.data.frame(cas_to_tab)
tab_to_cas <- tidyr::uncount(temp, weights = Freq)
head(tab_to_cas, 5)
```

-   table to count

```{r}
tab_to_freq <- as.data.frame(cas_to_tab)
head(tab_to_freq, 5)
```

-   count to table

```{r}
freq_to_tab <- xtabs(Freq ~ item + group,
                     tab_to_freq)
head(freq_to_tab, 5)
```

-   count to individuals

```{r}
freq_to_cas <- tidyr::uncount(
                cas_to_freq,
                weights = Freq)
head(freq_to_cas, 5)
```



---

## 실습 8. 모비율의 검정

-   다음은 자동화 생산의 핵심 공정인 [E단계]와 [R 단계]에서 조사된 불량의 개수이다. 조사 과정에서는 핵심 Para.s 또는 재료의 변경을 제한하였고, 이전 조사에 의하면 두 공정의 불량률은 거의 동일하다.
    -   1. 각 단계의 불량률이 0.07보다 작다고 할 수 있는지를 검정하시오.
    -   2. 현재 조사된 데이터를 이용하여 고려된 공정의 불량률이 동일한지를 확인하시오.

---

|             | E 단계 | R 단계 |
|:-----------:|:-------:|:-------:|
| Defect      |   72    |   28    |
| Non-Defect  |  928    |  672    |
| Total       | 1000    |  700    |

---

-   E단계

```{r}
prop.test(72, 1000, 0.07, alternative = "less")
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.6 > $alpha$=0.05 이므로 $H_{0}$ 기각 못한다.
-   E단계의 프로세스는 최대 8.7%의 불량률이 나타날 수 있다.

---

-   R단계

```{r}
prop.test(28, 658, 0.07, alternative = "less")
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.004 < $alpha$=0.05 이므로 $H_{0}$ 기각한다.
-   R단계의 프로세스는 최대 5.8%의 불량률이 나타날 수 있다.

---

-   두 단계의 불량률 비교

```{r}
defect <- c(72, 28)
n <- c(1000, 700)
prop.test(defect, n)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.008 < $alpha$=0.05 이므로 $H_{0}$ 기각한다.
    -   신뢰구간 또한 0값을 포함하고 있지 않다.
-   두 프로세스의 불량률은 동일하다고 볼 수 없다.
-   최소 0.9%, 최대 5.5의 차이를 보일 수 있다.
-   따라서 E단계의 공정률은 조정이 필요해 보인다.

---

## 실습 9. 모비율의 검정2, customer.ds.csv

-   다음은 특정 제품을 구매한 지 1년이 지난 고객 중에서 n=1000명을 선택하여 전화 조사를 통하여 디자인(design)과 서비스(service) 만족도를 5점 척도로 조사한 것이다.
-   (1) 4점 이상을 "만족"으로 가정한다
-   (2) 고려한 제품은 디자인과 서비스 중에서 어떤 특성이 더 만족도가 높은가?

-   데이터 불러오기

```{r}
ds <- read.csv(paste0(data_path, "customer.ds.csv"))
str(ds)
```

```{r}
# 막대그래프 생성 (dodge position)
ds1_plot <- ggplot(data = ds, aes(x = design, fill = factor(service), group = service)) +
  geom_bar(position = "dodge") +
  labs(x = "Design", y = "Freq", fill = "Service") +
  theme_bw()

# 막대그래프 생성 (fill position)
ds2_plot <- ggplot(data = ds, aes(x = design, fill = factor(service), group = service)) +
  geom_bar(position = "fill") +
  labs(x = "Design", y = "Percent", fill = "Service") +
  scale_y_continuous(labels = scales::percent) +
  theme_bw()

gridExtra::grid.arrange(ds1_plot, ds2_plot, ncol = 2)
```

-   만족 고객 구분 및 비교

```{r}
ds$d.satisfied <- ifelse(ds$design >= 4, "Yes", "No")
ds$s.satisfied <- ifelse(ds$service >= 4, "Yes", "No")
comparison <- table(ds$d.satisfied, ds$s.satisfied)

prop.test(comparison)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.7 > $\alpha$이므로 $H_{0}$기각 하지 못한다.
    -   따라서 두 특성의 만족도는 서로 다르다고 할 수 없다.

---

## 실습 10. 쌍체 데이터의 비율 비교

-   광고의 유용성을 확인하기 위하여 잠재 구매 고객을 두 개의 Group으로 나누고 광고를 보기 전에 제품을 구매할 것인가의 여부(buy, not buy) 그리고 동일한 고객이 광고를 접하고 난 후의 제품 구매 여부를 조사하였다. 이는 서로 다른 고객간의 의견 변동을 최소화 하기 위하여 설계되었다.
-   광고의 유용성 여부를 확인하시오.

| Before \ After | buy  | not buy | Total |
|----------------|------|---------|-------|
| buy            | 734  | 66      | 800   |
| not buy        | 96   | 704     | 800   |
| **Total**      | 830  | 770     | 1600  |

---


-   데이터 입력

```{r}
consumer <- matrix(c(734, 96, 66, 704), nrow = 2, ncol = 2)
dimnames(consumer) <- list(before.adv = c("buy", "not buy"),
                           after.adv = c("buy", "not buy"))
consumer
```

-   검정 진행하기

```{r}
mcnemar.test(consumer)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.02 < $\alpha$이므로 $H_{0}$기각한다.
    -   따라서 광고 전/후의 고객 구매 응답률은 서로 동일하지 않다.

---

## 실습 11. 지역/모델 별 고객 Claim 집계, claim.area.2019.csv

-   다음은 일정 기간 동안 조사된 제품(model) 별 Customer/Partner 불만 사항의 빈도를 지역 별로 집계한 것이다.

---

-   데이터 불러오기

```{r}
claim19 <- read.csv(paste0(data_path, "claim.area.2019.csv"))
str(claim19)
```


-   데이터 구조 확인

```{r}
my_color <- c(lre, sbl, ngr, lgo)
#
g1 <- claim19 %>% 
        ggplot(aes(x = model, y = frequency, fill = area)) +
          geom_bar(stat = "identity", position = "dodge") +
          labs(title = "Contingency Table",
               x = "모델명", y = "빈도") +
          scale_fill_manual(values = my_color) +
          theme_bw()
#
g2 <-   claim19 %>% 
          ggplot(aes(x = model, y = frequency, fill = area)) +
            geom_bar(stat = "identity", position = "fill") +
            labs(title = "Contingency Table",
                 x = "모델명", y = "빈도") +
            scale_fill_manual(values = my_color) +
            scale_y_continuous(labels = scales::percent) +
            theme_bw()
gridExtra::grid.arrange(g1, g2, ncol =2)
```


-   Extra Visualiation1

```{r}
claim19_t <- xtabs(frequency ~ model + area, claim19)
vcd::doubledecker(claim19_t)
```

-   카이제곱 독립성 검정
    -   모든 셀의 기대 도수(Expected)는 5보다 커야 한다. 그렇지 않으면 경고 메시지를 출력한다.
    
```{r}
result <- chisq.test(claim19_t)
result
```
```{r}
result$p.value
result$expected
```

-   Extra visualization2

```{r}
claim19_t <- xtabs(frequency ~ model + area, claim19)
vcd::mosaic(claim19_t,
            gp = shading_max,
            labeling = labeling_residuals,
            digits = 2
            )
```

---

-   데이터 부분 선택: 해외향

```{r}
claim19_2 <- claim19 %>% 
  dplyr::filter(area != "1Domestic")
claim19_t2 <- xtabs(frequency ~ model + area, claim19_2)
vcd::doubledecker(claim19_t2, spacing = spacing_highlighting,
                  main = "Customer Claims 2019")
```

```{r}
result_2 <- chisq.test(claim19_t2)
result_2
```
```{r}
result_2$p.value
result_2$expected
```

```{r}
mosaic(claim19_t2, gp = shading_max, labeling = labeling_residuals, digits = 2)
```




---

## 실습 12. 정규성 검정 Normality Test, mid.measure.csv

-   다음은 생산 프로세스의 중간 단계에서 수집된 데이터로 목표값은 19.0(Micrometer, 0.001mm)이다.

---

-   데이터 불러오기 및 구조 확인
```{r}
check <- read.csv(paste0(data_path, "mid.measure.csv"))
str(check)
```

-   목표값 달성 유무와 산포 확인

```{r}
mean(check$length)
sd(check$length)
```


```{r}
summary(check)
```

-   분포 확인

```{r}
check %>% 
  ggplot(aes(length, y = ..density..)) +
  geom_histogram(color = "white", fill = "gray") +
  geom_density(color = lre) +
  geom_density(color = sbl, adjust = 2) +
  geom_density(color = ngr, adjust = 0.5) +
  theme_bw()
```

-   Probability(Q-Q) Plot

```{r}
check %>% 
  ggplot(aes(sample = length)) +
  stat_qq(color = sbl) +
  stat_qq_line(color = lre) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 1, linetype = "dashed") +
  labs(title = "Probability(Q-Q) plot",
       x = "Theoretical quantiles(Standard Normal)",
       y = "Sample quantiles(Data)"
       ) +
  theme_bw()
```

-   정규성 검정

```{r}
ad.test(check$length)
pearson.test(check$length)
ks.test(check$length, "pnorm")
```

-   그룹 별 정규성 검정

```{r}
ad_tests <- lapply(split(check$length, check$set), ad.test)
p_values <- sapply(ad_tests, function(x) x$p.value)
p_values

check %>% 
  ggplot(aes(sample = length, color = set)) +
  geom_qq() +
  stat_qq_line(aes(color = set)) +
  labs(title = "Q-Q plot by sets",
       x = "Standard Normal", y = "Data") +
  theme_bw()
```

-   그룹 별 모수(들) 추정
    -   단일 그룹 변수의 범주 별로 모수(그리고 표준오차)를 추정

```{r}
mid <- pars_group(check, length, set, "normal")

mid %>% 
  bind_rows() %>% 
  mutate(mean = purrr::map_dbl(fit, "mean"),
         sd = purrr::map_dbl(fit, "sd")
         ) %>% 
  select(-fit)
```

-   what should we do now?

```{r}
check %>% 
  ggplot(aes(x = length, color = set)) +
  geom_density(adjust = 2) +
  theme_bw()
```

---

## 실습 13. 정규성 변환(Box Cox Transformation), auto.defect.csv

-   다음은 반복 자동화 테스트에서 최초 결점이 발생할 때 까지의 시간(sec.s)을 조사한 것이다.

---

-   데이터 불러오기 및 구조 확인

```{r}
defect <- read.csv(paste0(data_path, "auto.defect.csv"))
str(defect)
```

-   평균과 산포 확인

```{r}
mean(defect$times)
sd(defect$times)
```

-   정규정 검정

```{r}
ad.test(defect$times)
pearson.test(defect$times)
ks.test(defect$times, "pnorm")
```

-   Box-Cox Transformation

```{r}
x <- defect$times
bc <- boxcox(x ~ 1)
lambda <- round(bc$x[which.max(bc$y)], 2)
x_trans <- (x^lambda -1) / lambda
ad.test(x_trans)
```

-   변환 후 분포 확인

```{r}
defect$trans0.2 <- x_trans
```

-   변환 전 후 mean 값 비교하기
```{r}
mean <- mean(defect$times)
mean_by_trans <- ((mean(x_trans)*lambda) + 1)^(1/lambda)
c(mean, mean_by_trans)
```


---

## 실습 14. 신제품 인증 시험 검증, touch.ms.csv

-   신제품 인증 시험에서는 Touch 반응의 평균값의 230(ms)보다 작아야 한다.
    -   개선 활동의 검증에는 일반적으로 방향성 가설이 적절하다.

---

-   데이터 불러오기 및 구조 확인

```{r}
touch <- read.csv(paste0(data_path, "touch.ms.csv"))
str(touch)
```
-   기초 통계량 확인

```{r}
c(mean = mean(touch$ms), stdDev = sd(touch$ms), max = max(touch$ms))
```

-   정규성 검정

```{r}
ms <- touch$ms
test <- c(ad = ad.test(ms)$p.value, pearson = pearson.test(ms)$p.value)
round(test, 3)
```

```{r}
touch %>% 
  ggplot(aes(ms, y = ..density..)) +
  geom_histogram(color = "white", fill = "gray") +
  geom_density(color = lre) +
  geom_density(color = sbl, adjust = 2) +
  geom_density(color = ngr, adjust = 0.5) +
  theme_bw()
```

-   가설 검정 진행

```{r}
t.test(touch$ms, mu = 230, alternative = "less")
```
-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.07 > $\alpha$이므로 $H_{0}$기각할 수가 없다.
    -   결과를 살펴보면 표본 평균은 229로 나오지만, 모평균 $\mu$는 최대 230(ms)일 수 있다.
    -   따라서 기존 가설을 유지한다.

---

## 실습 15. 소음 개선 후 Pilot Sample 개선 검증, noise.db.csv

-   제품의 소음(noise) 개선 전(old)/후(new)를 비교하여 개선 정도를 확인하고자 한다.

---

-   데이터 불러오기 및 구조 확인

```{r}
noise <- read.csv(paste0(data_path, "noise.db.csv"))
str(noise)
```

-   표본 평균과 표본 표준편차 확인

```{r}
c(old = mean(noise$old),
  new = mean(noise$new, na.rm = TRUE))

c(old = sd(noise$old),
  new = sd(noise$new, na.rm = TRUE))

```
-   정규성 확인

```{r}
c(old = ad.test(noise$old)$p.value,
  new = ad.test(noise$new)$p.value)
```

-   등분산성 확인
  
```{r}
var.test(noise$old, noise$new)
```

-   t.test 진행

```{r}
t.test(noise$new, noise$old, alternative = "less", var.equal = TRUE)
```

-   $\alpha$값은 휴리스틱에 의해 0.05로 설정한다.
-   p-value = 0.0000000000000002 < $\alpha$=0.05이므로 $H_{0}$기각한다.


---

-   어느 정도로 성능이 개선되었느지 확인 진행

```{r}
alpha <- 0.05
for(Delta in seq(1, 30, by =1)){
  t.test_result <- t.test(noise$new + Delta,
                          noise$old,
                          alternative = "less",
                          var.equal =TRUE)
  if(t.test_result$p.value > alpha){
    cat("Delta = ", Delta, "에서 최초로 H_0를 기각하지 못한다.
        이때 p-value=", t.test_result$p.value, "\n")
    break
  }
}
```

-   평균 소음은 최대 약 19.6(db) 정도 개선되었다.
    -   이렇게 분석을 진행하는 것이 방향성 가설이다.

---

## 실습 16. 쌍체 데이터의 차이의 모평균 비교, exposure.con.csv

-   노출 시간은 표본 변이에 민감하며, 결과인 지적도(Concentration;C/m^2)에 영향을 준다.
-   연구자는 먼저 일정 수의 표본을 랜덤하게 선택한 후 각 표본에 노출 시간 1 min을 적용하여 집적도를 먼저 측정하고 동일한 표본에 2min.s 추가로 적용하여(총 3 min.s)집적도를 재측정하였다.
-   만일 노출 시간에 따른 집적도가 2.0(C/m^2)이상 차이가 난다면, 노출 시간을 3 min.s으로 변경하고자 한다.

---

-   데이터 불러오기

```{r}
con <- read.csv(paste0(data_path, "exposure.con.csv"))
str(con)
```
-   쌍체 검정의 유용성 확인

```{r}
cov(con$x1m, con$x3m)
cor(con$x1m, con$x3m)
```

-   수집된 데이터는 강한 양(+)의 상관 관계를 갖는다.

```{r}
con %>% ggplot(aes(x = x1m, y = x3m)) +
  geom_point(color = sbl) +
  labs(title = "paired data (x1m, x3m)") +
  theme_bw()
```

-   히스토그램과 정규확률그림

```{r}
con$diff <- con$x3m - con$x1m
hist_density(con, diff)
qq_normal(con, diff)
```

-   히스토그램과 정규확률그림을 봤을 때 정규성을 만족하는 것으로 보인다.

---

-   쌍체 차이의 정규성 검정

```{r}
ad.test(con$diff)
```
-   p-value = 0.3 > $\alpha = 0.05$이므로 $H_{0}$을 기각하지 못하여 정규분포를 따를 가능성이 있다고 판단한다.

---

-  쌍체 검정

```{r}
t.test(con$x3m, con$x1m, 2.0, alternative = "greater", paired = TRUE)
```

-   p-value = 0.007 < 0.05 이므로 $H_{0}$를 기각한다.

---

-   2 sample t-test

```{r}
t.test(con$x3m, con$x1m, 2.0, alternative = "greater")
```

-   p-value = 0.2 > 0.05 이므로 $H_{0}$를 기각하지 못한다.

---

-   paired t-test를 적용할 경우에는 노출 시간을 제외하고는 나머지 변동성은 통제하기 때문에 이렇게 2 sample t-test와 차이를 가지게 된다.


---

## 실습 17. 분산 분석, depth.etch.csv

-   다음은 새로운 식각 방법(method)들에 대하여 조사된 특정 위치의 깊이(depth)이다.
-   식긱 방법 중 Control은 현재 사용하는 방법이며, (A1, A2)는 세정액의 서로 다른 배합 그리고 (B1, B2)는 가스 노출 시간을 서로 다르게 지정한 것이다.
-   가장 큰 식각 깊이를 제공하는 방법을 선택하고자 한다.

---

-   데이터 불러오기

```{r}
etch <- read.csv(paste0(data_path, "depth.etch.csv"))
str(etch)
```

-   데이터는 총 225개의 관측치와 2개의 변수로 구성
    -   **`method`:** 식각 방법을 나타내는 문자열 변수
    -   **`depth`:** 해당 방법으로 식각했을 때의 깊이를 나타내는 숫자형 변수

---

-   데이터 확인

```{r}
g1 <- etch %>%
  ggplot(aes(x = method, y = depth)) +
  geom_boxplot() +
  geom_jitter(color = sbl) +
  theme_bw()

g2 <- etch %>% 
  ggplot(aes(x = method, y = depth)) +
  geom_violin() +
  geom_jitter(color = lre) +
  theme_bw()
gridExtra::grid.arrange(g1, g2, ncol = 2)
```

-   ANOVA 진행

```{r}
model <- aov(depth ~ method, data = etch)
summary(model)
```

-   p-value = 0.0000000000000002 < 0.05 이므로 $H_{0}$를 기각한다.
    -   따라서 고려된 식각 방법이 모두 동일하지는 않다.

---

-   수준 별 요약값 정리

```{r}
model.tables(model, tpye = "means", se = TRUE)
```

-   각 방법의 효과의 크기를 나타낸다.
    -   `Control` 방법은 전체 평균보다 약 30.06만큼 낮은 것을 알 수 있다.
    -   `B2` 방법은 전체 평균보다 약 28.83보다 높은 것을 알 수 있다.
        -   따라서 `B2' 방법이 가장 깊은 식각을 제공한다.

---

-   잔차 진단

```{r}
etch$res <- resid(model)
etch$rsd <- rstandard(model)
etch$fit <- fitted.values(model)
m_residuals(etch)
```

-   Quantile-Quantile (Q-Q) 그래프 (왼쪽 상단 그래프)
    -   잔차가 정규 분포를 따른다면 점들이 직선을 이루게 된다.
    -   대부분의 점들이 빨간선 주변에 위치하고 있다. 하지만 극단값에서는 라인에서 약간 벗어나 있다.
        -   따라서 꼬리 부분에서 이상치의 가능성이 있다.

-   잔차 vs 예측값 그래프 (오른쪽 상단 그래프)
    -   일정한 주기로 반복되는 패턴을 보인다.

-   잔차의 히스토그램 (왼쪽 하단 그래프)
    -   대체로 히스토그램의 종 모양에 가깝다.

-   잔차 vs 순서 그래프 (오른쪽 하단 그래프)
    -   잔차의 변동성이 커보인다.


---

-   표준화 잔차의 정규성 진단

```{r}
ad.test(etch$rsd)
```

-   p-value = 0.08 > 0.05 이므로 $H_{0}$를 기각한다.
    -   따라서 정규성에 위배되지 않는다.


---

# 선형 회귀 분석 Linear Regression Analysis

-   회귀 계수는 통계량이다.
-   회귀 분석은 알려져 있지 않은 f(x)를 대상으로 적용된다.
-   모수 절약(Parsimony)의 원칙 하에서 과현상에 대한 해석력(Interpretation)을 확보해야 한다.
-   선형 회귀 분석 Linear Regression Analysis
    -   **[동시]{style="color: red;"}**에 측정했을 때 사용**[(같은 공간, 시간일 때)]{style="color: red;"}**
-   y: **[반응 변수(response)]{style="color: red;"}**, 종속 변수
-   x: **[설명 변수(exploratory variable(s))]{style="color: red;"}**, 예측 변수, 독립변수

# 4. 단순 회귀 분석 Simple Regression Analysis

$$
y_i = \beta_0 + \beta_1x_i + \varepsilon_i \quad (i = 1,2,...,n),~~\varepsilon_i \sim iid \ N(0,\sigma^2)
$$

-   (1) 오차는 상호 독립이다.
-   (2) 오차는 정규 분포(normal distribution)를 따른다.
-   (3) 오차는 모든 설명 변수의 값에 대하여 분산 \(\sigma^2\)가 같다.
    -   iid; independently identically distributed

---

-   데이터를 이용한 회귀 계수(들)와 분산의 추정

$$
(x_i,y_i), \ i = 1,2,...,n \rightarrow (\beta_0, \beta_1, \sigma^2)
$$

---

-   추정된 회귀식(estimated regression equation)
    -   $\hat{y} = y_0$은 평균 반응(mean response)으로 해석
    -   고려된 데이터의 범위 내에서 적용

$$
\hat{y} = \beta_0 + \beta_1x_i,~~~x \in [x_{min}, x_{max}]
$$ 

---

-   \(i\)th 잔차(residual); \( \varepsilon_i \)
    -   가정된 모형 하에서 오차의 추정값
    -   관측값 \( y_i \)와 추정된 회귀선 \( \hat{y}_i \)의 차이

$$
\varepsilon_i = y_i - \hat{y}_i = y_i - (\beta_0 + \beta_1x_i), \quad (i = 1,2,...,n)
$$

## 4.1. 회귀 계수의 추정: 최소 제곱법(Method of Least Squares)

$$\arg \min_{\beta_0, \beta_1} L(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2$$

-   최소 제곱 추정량(LSE)

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$
$$\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}$$

여기서,

$$S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$$
$$S_{xy} = \sum_{i=1}^{n} (y_i - \bar{y})(x_i - \bar{x})$$

-   LSE의 통계적 특성

$$E(\hat{\beta}_0) = \beta_0, \quad Var(\hat{\beta}_0) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{S_{xx}} \right)$$
$$E(\hat{\beta}_1) = \beta_1, \quad Var(\hat{\beta}_1) = \frac{\sigma^2}{S_{xx}}$$

---

## 4.2. 잔차 residuals

-   잔차에는 설명 가능한 특정 패턴이 남아 있지 않아야 한다.

---

## 4.3. 변동의 분해 decompostion of variations

-   적합한 $R^2$를 판단하기 위한 통계적 기준값은 없다.
    -   ex) 0.64이상
-   결정 계수만으로 올바른 회귀 모형을 결정할 수 없다.

---

## 4.4. 회귀 계수의 유의성 검정

-   일반적으로 $\beta_0$의 유의성 검정 결과는 고려하지 않는다.
-   $\beta_0 = 0$을 적용한 경우를 원점을 통과하는 회귀 모형이라고 한다.
-   $\beta_1$의 유의성 검정 결과는 가정된 모형의 타당성에 종속된다.

---

## 4.5. 추정된 회귀식의 활용

-   예측 구간(prediction interval)은 항상 신뢰 구간(confidence interval)보다 넓거나 같다.
-   만일 일정 범위에서 가능한 모든 x 값에 대하여 구해진 신뢰구간과 예측 구간을 각각 신뢰대(confidence bands) 그리고 예측대(prediction bands)라고 부른다.

---

## 실습 17. 임의로 선택된 소형 화물의 무게$(x;~g)$과 부피$(y;~cm^3)$, simple.csv

-   예제1 simple.csv
-   무게에 의하여 운송 요금을 결정하는 소형 화물들 중에서 임의로 선택된 58개의 무게(x; weight)과 부피(y; volume)을 조사하였다.
-   이를 이용하여 중량과 부피간의 관계를 추정하여 운송 계획을 수립하고자 한다.

---

-   데이터 불러오기 및 확인

```{r}
cargo <- read.csv(paste0(data_path, "simple.csv"))
str(cargo)
```

-   결측값 확인

```{r}
na_count(cargo)
```
-   세 개의 변수 모두 결측값이 없다.

---

-   LS 추정량 계산
    -   **[round(x, digits = k)]{style="color: blue;"}**는 유효 숫자를 소수점 k번째로 반올림한다.

```{r}
x <- cargo$x
y <- cargo$y
Sxy <- sum((x - mean(x)) * (y - mean(y)))
Sxx <- sum((x - mean(x))^2)
Syy <- sum((y - mean(y))^2)
#
hat_beta1 <- Sxy / Sxx
hat_beta0 <- mean(y) - hat_beta1*mean(x)
round(c(hat_beta0, hat_beta1), 2)
```

-   추정된 회귀 계수 $\hat\beta_0 = -204.1$ 그리고 $\hat\beta_1=31.7$로 계산되었다.

---

-   simple_reg()함수를 통한 분석

```{r}
simple_reg(cargo, x, y)
```

-   결측치는 존재하지 않는다.
-   결정계수값은 0.68로 잘 적합하고 있다고 볼 수 있다.
-   p-value = 0.00000000000000162 < $\alpha = 0.05$ 이므로 $H_{0}$를 기각하므로 통계적으로 유의하다.
    -   위 회귀모형은 적절하다.
-   신뢰대는 빨간색 점선으로 이루어져 있다.
    -   신뢰대가 좁게 형성된 것으로 보아, 회귀선의 위치에 대한 불확실성이 적은 것으로 보인다.
-   예측대를 확인해봤을 때 일부 이상치가 있는 것으로 보인다.

---

## 실습 18. 유통 매장 정리 시간, dist.store.csv

-   00유통에서는 물품의 수에 따른 매장 정리 시간의 기준을 결정하기 위하여 다음의 조사를 실시하였다.
    -   (1) 일상적인 정리업무를 하는 직원 중에서 랜덤하게 15명을 선택
    -   (2) 각 직원 별 랜덤하게 할당된 서로 다른 물품의 수를 정리하는데 소요되는 정리 시간(min.s)을 기록
-   이 때 물품의 수는 지난 2개월간 정리된 물품 수의 분포로부터 랜덤하게 선정하였다.
-   참고로 정리하기 위하여 이동한 총 거리(m) 역시 함께 기록되었다.

---

-   데이터 불러오기

```{r}
dist <- read.csv(paste0(data_path, "dist.store.csv"))
```

---

-   데이터 확인

```{r}
str(dist)
head(dist)
tail(dist)
view(dist)
```


-   변수명을 소문자로 변환

```{r}
dist <- dist %>% 
  rename_all(tolower)

head(dist, 5)
```

-   각 변수별 결측값 확인하기

```{r}
na_count(dist)
```

-   세 변수 모두 결측값이 존재하지 않는다.

---

-   시각화: 관계의 검토

```{r}
dist %>% 
  ggplot(aes(case, time)) +
  geom_point(color = sbl) +
  theme_bw()
```

-   (case, time)의 관계는 직선의 경향을 갖는다.

---

-   회귀 분석의 실행

```{r}
dist_fit <- lm(time ~ case, data = dist)
```

-   분석 결과 확인

```{r}
summary(dist_fit)
```

-   p-value = 0.00000000000000822 < $\alpha = 0.05$ 이므로 $H_{0}$를 기각한다.
    -   회귀 모형은 유의하다.
-   `case`회귀 계수 또한 p-value < $\alpha = 0.05$로 유의하다.
-   현재 모형의 설명력은 0.93으로 높은 편이다.

---

-   회귀계수 확인

```{r}
summary(dist_fit)$coefficients
```

-   잔차 확인

```{r}
summary(dist_fit)$residuals
```

-   `coef()`, `fitted()`, `resid()`: 회귀 계수, 추정값, 잔차 확인 가능

```{r}
coef(dist_fit) #추정된 회귀 계수 추출
```

```{r}
fitted(dist_fit) #현재 데이터를 이용한 추정값
```

```{r}
resid(dist_fit) #잔차
```

-   표준화 잔차의 절대값이 2보다 크거나 같은 경우만을 선택

```{r}
rsd <- rstandard(dist_fit)
outliers_d <- as.numeric(names(rsd)[abs(rsd) >= 2])
dist[outliers_d, ]

dist_1 <- dist[-outliers_d, ]
```

-   스튜던트화 잔차의 절대값이 3보다 크거나 같은 경우만을 선택

```{r}
rst <- rstudent(dist_fit)
outliers_t <- as.numeric(names(rst)[abs(rst) >= 3])
dist[outliers_t, ]
```

-   $R^2_{pred}$의 계산

```{r}
PRESS <- sum(rstandard(dist_fit, type="pred")^2)
SST <- sum((dist$time - mean(dist$time))^2)
r.sq.p <- 1-(PRESS/SST)
paste("predicted R^2 = ", round(r.sq.p, 2))
```

-   잔차 그림

```{r}
rsd <- rstandard(dist_fit)
fit <- fitted.values(dist_fit)
dist_after <- cbind(dist, rsd, fit)

m_residuals(dist_after)
```


-   모형 적합 후 표준화 잔차와 적합치를 보완한 후 `m_residuals(df)` 실행

---

-   `geom_smooth()`는 사전 분석 없이 기본 그림에 추정된 회귀식을 함께 도시해준다.

```{r}
dist %>% 
  ggplot(aes(x = case, y = time)) +
  geom_point(color = sbl) +
  stat_smooth(method = "lm",
              formula = y~x,
              se = FALSE,
              color = lre
              ) +
  theme_bw()
```

-   평균 반응값의 구간 추정

```{r}
new_case <- data.frame(case = c(20, 30))
predict(dist_fit, newdata = new_case,
        interval = c("confidence"), level = 0.95)
```

-   새로운 관찰값에 대한 평균 반응값의 구간 추정

```{r}
new_case <- data.frame(case = c(20, 30))
predict(dist_fit, newdata = new_case,
        interval = c("prediction"), level = 0.95)
```

-   신뢰대(confidence bands)와 예측대(prediction bands)
    -   ggfortify::는 통계 분석 결과를 시각화해주는 다양한 도구를 제공

```{r}
# ggfortify 패키지 설치 및 불러오기
# install.packages("ggfortify")
library(ggfortify)
library(ggthemes)

# 데이터 생성
case_grid <- seq(min(dist$case), max(dist$case), by = 0.01)

# 신뢰구간(confidence bands) 예측
dist_c_band <- predict(dist_fit,
                       newdata = data.frame(case = case_grid),
                       interval = "confidence", level = 0.95)

# 예측구간(prediction bands) 예측
dist_p_band <- predict(dist_fit,
                       newdata = data.frame(case = case_grid),
                       interval = "prediction", level = 0.95)

# 그래프 그리기
ggplot(dist_fit, aes(x = case, y = time)) +
  geom_point(color = sbl) +
  geom_smooth(method = lm, se = TRUE) + # se=TRUE를 선택하여 신뢰 구간을 표시할 수 있다.
  geom_point(aes(x = mean(case), y = mean(time)), 
             color = lgo, 
             size = 3) +
  geom_line(data = dist_c_band, 
            aes(x = case_grid, y = lwr), 
            color = lre, linetype = "dashed") +
  geom_line(data = dist_c_band, 
            aes(x = case_grid, y = upr), 
            color = lre, linetype = "dashed") +
  geom_line(data = dist_p_band, 
            aes(x = case_grid, y = lwr), 
            color = "darkgray", linetype = "dashed") +
  geom_line(data = dist_p_band, 
            aes(x = case_grid, y = upr), 
            color = "darkgray", linetype = "dashed") +
  labs(title = "추정된 회귀선과 신뢰대/예측대",
       subtitle = paste("meantime =", round(dist_fit$coefficients[1], 2), 
                        "+", round(dist_fit$coefficients[2], 2), "case", 
                        ", R^2 =", round(summary(dist_fit)$r.squared, 2))) +
  theme_economist() +
  scale_color_economist()
```


-   `case`에 따라 `time`이 어떻게 변화하는지를 나타내는 회귀선이다.
-   `case`값이 1씩 증가할때마다 `time`이 2.18단위로 증가한다는 것을 의미한다.
-   결정계수값도 0.93으로 높은 값이라고 볼 수 있다.
    -   `case`와 `time`사이에는 선형 관계가 있어보이고, 회귀식은 잘 적합되어 있어 보인다.

---

## 실습 19. Test Case Determination, test.sw.csv

-   00전자의 SWD팀은 최근 개발된 모델들을 중심으로 "KLOC"에 따른 적절한 'Test Case'의 수에 대한 기준을 결정하기 위하여 기존 개발된 모델에서 두 변수간 밀접한 관계가 있는지를 확인하고, 이에 대한 모형을 설정하고자 한다.
    -   (1). 각 변수의 결측값(NA)의 수를 확인하시오.
    -   (2). 단순 회귀 모형을 가정하고 회귀 계수의 유의성 검정 결과를 제시하시오.
    -   (3). 결정 계수의 값을 제시하시오.
    -   (4). 잔차 진단을 실시하고 이상점 유무를 확인하시오.
    -   (5). 신뢰대와 예측대를 포함하여 단순 회귀 분석의 결과를 요약하시오.

---

-   데이터 불러오기 

```{r}
swtest <- read.csv(paste0(data_path, "test.sw.csv"))
```

-   사용자 정의 함수 적용(단순 회귀 분석)

```{r}
swtest <- data.pre(swtest)
```

-   분석을 용이하게 하기 위하여 변수명을 소문자로 변환하였다.

---

-   잔차 진단

```{r}
swtest_fit <- lm(ttcase ~ kloc, data = swtest)

rsd <- rstandard(swtest_fit)
fit <- fitted.values(swtest_fit)
swtest_after <- cbind(swtest, rsd, fit)
m_residuals(swtest_after)
```

-   일부 양끝단을 제외하고는 정규성을 만족하는 거승로 보인다.

---

-   정규성 검정 test

```{r}
ad.test(swtest_after$rsd)
pearson.test(swtest_after$rsd)
ks.test(swtest_after$rsd, "pnorm")
```

-   세가지의 정규성 테스트를 해본 결과 p-value > $\alpha = 0.05$ 이므로 $H_{0}$를 기각하지 못하였다.
    -   정규성을 만족하는 것으로 보인다.

---

-   단순 회귀 분석 결과

```{r}
simple_reg(data = swtest, swtest$kloc, swtest$ttcase)
```

-   결측치는 존재하지 않는다.
-   결정계수값은 0.766로 잘 적합하고 있다고 볼 수 있다.
-   p-value = 0.0000000000000002 < $\alpha = 0.05$ 이므로 $H_{0}$를 기각하므로 통계적으로 유의하다.
    -   위 회귀모형은 적절하다.
-   신뢰대는 빨간색 점선으로 이루어져 있다.
    -   신뢰대가 좁게 형성된 것으로 보아, 회귀선의 위치에 대한 불확실성이 적은 것으로 보인다.
-   예측대를 확인해봤을 때 모델이 데이터를 잘 반영하고 있다고 볼 수 있다.

---


# 5. 다중 회귀 분석 Multiple Regression Analysis

## 5.1. 최소 제곱법과 중요 결과

$$\arg \min_{\beta_0,\beta_1,\beta_2,...,\beta_{p-1}} L(\beta_0,\beta_1,\beta_2,...,\beta_{p-1})$$

여기서,

$$L (\beta_0,\beta_1,\beta_2,...,\beta_{p-1}) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} - ... - \beta_{p-1} x_{i(p-1)})^2$$
$$= (y - X\beta)^T (y - X\beta)$$

-   회귀 계수의 LS추정량

$$\hat{\beta} = (X^T X)^{-1} X^T y$$
$$E(\hat{\beta}) = \beta \quad \text{그리고} \quad Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1}$$

-   반응값의 추정량과 잔차

$$\hat{y} = X\hat{\beta}, \quad e = y - \hat{y} \quad$$
$$\quad \hat{y} = E(y|X) = X \hat{\beta} = X(X^T X)^{-1} X^T y$$

-   $\sigma^2$의 추정량 $s^2_{e}$

$$s^2_{e} = \frac{1}{n-p} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{e^T e}{n-p}$$

---

## 5.2. 변수 선택 Variable Selection

-    예측자(predictors)에 대한 사전 정보가 부족하여 적절한 예측자를 선정하지 못하거나
-   수집된 데이터에서 예측자가 많은 경우 적용
-   AIC, BIC 모두 값이 작을수록 바람직하다.
    -   AIC; Akike Information Criterion
    -   BIC; Bayesian Information Criterion
-   2개 이상의 후보 모형을 선정하고, 잔차 분석 등의 추가 진단을 통하여 최종 모형을 선택한다.
    -   **[memisc::mtable()]{style="color: blue;"}**: 모형의 비교 결과를 요약
-   Mallow's $C_{p}$
    -   회귀 모델 선택에서 사용되는 통계량
    -   여러 후보 회귀 모델 중에서 가장 적절한 모델을 선택하기 위한 기준으로 사용
    -   이 지표도 작을수록 잘 적합했다고 볼 수 있다.
    

### 5.2.1. 전방 선택 Foreward Selection

-   상수 모형에서 기여도가 큰 순서대로 Predictor들을 차례로 선택한다.
-   전 단계에서 모형에 포함된 Predictor들은 계속 모형에 남게 된다.
-   예측자(predictors) 들의 나열 순서는 영향을 주지 않는다.
    -   **[stat::step]{style="color: blue;"}**은 AIC를 기준으로 변수를 선택한다.
    -   BIC를 기준으로 변수를 선택하고자 하는 경우, k = long(n)을 옵션에 추가한다.
    -   전방 선택(direction = "forward")의 경우, 시작 모형을 지정해야 한다.
    -   상수 모형인 경우 y ~ 1 그리고 반드시 포함해야 할 예측자가 있는 경우, y ~ x1 + x2..로 지정하여 사용한다.


### 5.2.2. 후방 제거 Backward Elimination

-   완전 모형에서 가장 기여도가 작은 순서대로 Predictor들을 차례로 제거한다.
-   전 단계에서 모형에서 제거된 예측자들은 모형에 다시 포함될 수 없다.
    -   후방 제거(direction = "backward")의 경우, full model에서 시작하므로 시작 모형을 지정하지 않는다.

### 5.2.3. 단계별 선택 Stepwise Selection

-   후방 소거의 개념을 포함하는 전방 선택법으로 각 예측자들을 선택한 다음에 제거될 필요가 있는 예측자들을 검토하여 제거한다.
-   일단 모형에 포함된 예측자들도 다시 모형에서 제거될 수 있다.
-   단계적 선택법은 후방제거를 보완한 전방 선택법이다. 따라서 시작 모형을 반드시 지정해야 한다.

### 5.2.4. 모든 부분 집합 

-   모든 가능한 회귀 모형을 적합하고, 예측자의 수에 따라 1개의 좋은 모형을 나열해 준다.
-   특정 모형을 지정해 주지 않는다.
    -   **[regsubsets()]{style="color: blue;"}**을 이용하여 모든 부분 집합에 대한 회귀 모형을 고려한다.
        -   여기서, nvmax = k는 검토할 부분 집합의 최대값이다.


---

## 실습 20. C-materials, cem.materials.csv

-   다음은 C-material의 구성 선분에 따른 발생 열량 관계를 분석하고자 조사된 데이터이다.
-   먼저 열량에 중요한 영향을 주는 구성 성분을 구별해 내고, 이를 통하여 발생 열량의 정도를 추정할 수 있다면, 응고 시간을 예측할 수 있어 조립 절차를 크게 간소화 할 수 있다고 한다.
-   구성 성분을 조절하여 실험하기에는 시간과 비용이 만힝 소요되어, 상대적으로 변동이 큰 표본들을 구분하여 총 4가지의 구성 성분(t1, t2, t3, t4)을 수집하였다.
-   반응값은 C-material 1g 내의 열량을 나타낸다.

---

-    데이터 불러오기

```{r}
cem <- read.csv(paste0(data_path,"cem.materials.csv"))
str(cem)
```

-   변수 이름과 위치 정리

```{r}
cem <- cem %>% 
  dplyr::rename_all(tolower) %>% 
  select(-id) %>% 
  relocate(y)
```

-   결측값 확인

```{r}
na_count(cem)
```

-   데이터 시각화: 행렬도(matrix plot)
    -   lower는 matrix plot의 lower triangle을 말하고 upper는 upper triangle을 말한다.
    -   point, smooth, cor(relation)을 선택 가능
    -   diag는 대각 요소를 말함
    -   densityDiag, barDiag, blankDiag를 선택 가능

```{r}
cem %>% 
  ggpairs(columns = 1:ncol(.),
          lower = list(continuous = wrap("points", # continuous = "smooth"
                                         color = sbl,
                                         aplha = 0.5)
                   ),
          upper = list(continuous = wrap("points", # wrap("cor", size = 2.5)
                                         color = sbl,
                                         alpha =0.5)
                   ),
          diag = list(continuous = wrap("densityDiag",
                                        color = "black",
                                        alpha = 0.5)
                   ),
          title = "Component Data") +
  theme_bw()
```

-   함수 정의
    -   원하는 형태의 행렬도(matrix plot) 형식을 결정하여 함수화
    -   gg_mplot(data, title)
    
```{r}
gg_mplot <- function(df, ttl){
  df %>% 
  ggpairs(columns = 1:ncol(.),
          lower = list(continuous = wrap("points", #continuous = "smooth"
                                         color = sbl,
                                         alpha = 0.5)
                       ),
          upper = list(continuous = wrap("points", #wrap("cor", size = 2.5)
                                         color = sbl,
                                         alpha = 0.5)
                       ),
          diag = list(continuous = wrap("densityDiag",
                                        color = "black",
                                        alpha = 0.5)
                       ),
          title = ttl) +
  theme_bw()
}

ttl = "Component Data: function 활용"
gg_mplot(cem, ttl)
```

---

-   다중 회귀 분석: lm()
    -   lm() 함수는 단순 회귀 분석과 동일하게 적용된다.
    -   단, formula를 사전에 지정하는 경우에는 문자형으로 정의되어야 한다.

```{r}
fml <- "y ~ t1 + t2 + t3 + t4"
cem_model <- lm(fml, cem)
summary(cem_model)
```

-   모든 변수가 유의하지 않지만, $R^2 = 0.982$이다.

---

-   회귀 분석 관련 추정값 추출

```{r}
cem_model$coefficients
summary(cem_model)$sigma
```


---

-   구간 추정
    -   lm()에 의한 model object가 생성
        -   신뢰도(level = )하에서 **[stat::confint()]{style="color: blue;"}**를 이용하여 개별 회귀 계수에 대한 구간 추정 결과를 확인
        -   **[conflict()]{style="color: blue;"}**의 기본 함수 구조
            -   특정 회귀 계수(들)만을 원하는 경우, parm의 위치에 문자형으로 지정

```{r}
confint(cem_model, level = 0.95)
```

```{r}
confint(cem_model, c("t1", "t2"), level = 0.95)
```

-   가설 검정
    -   lm()을 적용한 model object의 summary()의 coef를 부분 선택
  
```{r}
summary(cem_model)$coef
```

-    예측 구간
    -   **[stat::predict()]{style="color: blue;"}**:model object(class = lm)를 이용하여 점 추정과 구간 추정을 실행 
        -   interval = c("none", "confidence", "prediction")
        -   "confidence"는 평균 반응값에 대한 신뢰구간을 제공
        -   "prediction"은 새로운 관찰값에서의 반응값에 대한 신뢰구간을 제공

```{r}
x0 <- data.frame(t1 = 11, t2 =56, t3 = 9, t4 = 20)
```

```{r}
#점추정
predict(cem_model, newdata = x0)
```
```{r}
#구간 추정: 신뢰 구간
predict(cem_model, newdata = x0, interval = "confidence", level = 0.95)
```
```{r}
#구간 추정: 예측 구간
predict(cem_model, newdata = x0, interval = "prediction", level = 0.95)
```

---

### 변수 선택 실습

#### 전방 선택

-   선택 과정의 정보를 생략하고 최종 경과만을 확인하고 싶으면 trace = 0 옵션을 추가한다.

```{r}
# fml <- "y ~ t1 + t2 + t3+ t4"
model_start <- lm(y ~ 1, data = cem)
cem_fore_aic <- step(model_start, scope = fml, diretion = "forward")
```


-   최종 선택된 모형의 회귀 계수의 수(p)와 AIC 제공

```{r}
extractAIC(cem_fore_aic)
```

-   추정된 회귀 계수들

```{r}
coef(cem_fore_aic)
```

-   Adj $R^2$

```{r}
summary(cem_fore_aic)$adj.r.squared
```

-   BIC

```{r}
cem_fore_bic <- step(model_start, scope = fml, direction = "forward",
                     k = log(length(cem)),
                     trace = 0)
coef(cem_fore_bic)
```

-   Adj $R^2$

```{r}
summary(cem_fore_bic)$adj.r.squared
```

-   최종 모형의 formula확인

```{r}
formula(cem_fore_bic)
```

---

#### 후방 제거

-   full model에서 시작하기 때문에 시작 모형을 지정하지 않는다.

---

-   AIC

```{r}
cem_back_aic <- step(cem_model, direction = "backward", trace = 0)
extractAIC(cem_back_aic)
```

```{r}
coef(cem_back_aic)
```

-   AIC를 기준으로 최종 모형의 formula

```{r}
formula(cem_back_aic)
```

---

-   BIC

```{r}
n <- length(resid(cem_model))
```

```{r}
cem_back_bic <- step(cem_model, direction = "backward",
                     k = log(n),
                     trace = 0)
coef(cem_back_bic)
```

-   BIC를 기준으로 최종 모형의 formula

```{r}
formula(cem_back_bic)
```

---

#### 단계별 선택

-   시작 모형을 반드시 지정해야 한다.

```{r}
cem_both_aic <- step(model_start, scope = fml, diretion = "both", trace = 0)
cem_both_bic <- step(model_start, scope = fml, diretion = "both",
                     k = log(n),
                     trace = 0)
```

-   단계별 선택법에 의하여 선택된 최종 모형의 formula

```{r}
formula(cem_both_aic)
```

---

-   AIC와 BIC기준으로 각각의 변수 선택 방법에 의하여 도출된 최종 모형을 비교

```{r}
final.model_aic <- rbind("forward" = as.character(formula(cem_fore_aic))[3],
                         "backward" = as.character(formula(cem_back_aic))[3],
                         "stepwise" = as.character(formula(cem_both_aic))[3]
                         )
final.model_bic <- rbind("forward" = as.character(formula(cem_fore_bic))[3],
                         "backward" = as.character(formula(cem_back_bic))[3],
                         "stepwise" = as.character(formula(cem_both_bic))[3]
                         )
info <- cbind(final.model_aic, final.model_bic)
colnames(info) <- c("AIC", "BIC")
info
```

-   AIC에 기초하여 선택된 모형의 예측자는 3가지 방법 모두 (t1, t2, t4)로 동일하다.
-   BIC에 기초하여 선택된 예측자는 (t1, t2, t4)와 (t1, t2)이다.
-   결과가 비슷하다면, 가능한 한 간단한 모형을 선택하는 것이 바람직하다.

---

-   AIC에 의하여 선택된 최종 모형의 결과는 다음과 같다.

```{r}
extractAIC(cem_fore_aic)
extractAIC(cem_back_aic)
extractAIC(cem_both_aic)
```

```{r}
summary(cem_back_aic)
```
-   세 가지 방법 모두 AIC가 동일하다.

---

-   BIC도 각각의 값들을 확인을 해보았다.

```{r}
extractAIC(cem_fore_bic)
extractAIC(cem_back_bic)
extractAIC(cem_both_bic)
```

-   BIC에 의하여 선택된 최종 모형의 결과는 다음과 같다.

```{r}
summary(cem_back_bic)
```

-   이 모형같은 경우에는 더 적은 변수로 회귀식을 도출할 수 있다.
-   다른 방법같은 경우에도 비슷한 BIC를 보이므로 잔차 분석을 통한 추가 분석도 고려해 볼 필요가 있다.

---

#### 모든 부분 집합

-   `library(leaps)`
    -   **[regsubsets()]{style="color: blue;"}**을 이용하여 모든 부분 집합에 대한 회귀모형을 고려한다.
        -   nvmax = k는 검토할 부분집합의 최대값
    -   **[memisc::mtable()]{style="color: blue;"}**: 모형의 비교 결과를 요약한다.
    
```{r}
library(leaps)

temp <- leaps(x = cem[, -1], y = cem[ , 1])
full <- as.formula(paste("y ~ ",
                         paste(colnames(cem[-1]), collapse = "+"))
                   )
cem_all <- regsubsets(full, data = cem, nvmax = 4)
summary(cem_all)
```

```{r}
model.q <- summary(cem_all)
```

모형 선택중

```{r}
# 수정 결정계수가 가장 큰 모형
(adj.r2 <- which.max(model.q$adjr2))
# Cp가 가장 작은 모형
(cp <- which.min(model.q$cp))
# BIC가 가장 작은 모형
(bic <- which.min(model.q$bic))
```

-   2개 이상의 후보 모형을 선정하고, 잔차 분석 등의 추가 진단을 통하여 최종 모형을 선택

```{r}
# install.packages("memisc")

library(memisc)

model_1 <- lm(formula = y ~ t1, data = cem)
model_2 <- lm(formula = y ~ t1 + t2, data = cem)
model_3 <- lm(formula = y ~ t1 + t2 + t3, data = cem)
model_4 <- lm(formula = y ~ t1 + t2 + t3 + t4, data = cem)

s_table <- mtable("M1" = model_1, "M2" = model_2, "M3" = model_3, "M4" = model_4,
                  summary.stats = c("sigma", "R-squared", "F", "P"))
s_table
```

-   $Mallow's~C_{p}$는 다음을 이용하여 계산한다.

```{r}
# install.packages("olsrr")
library(olsrr)

full <- as.formula(paste("y ~ ",
                         paste(colnames(cem[-1]), collapse = "+"))
                         )
full

temp <- as.formula(paste("y ~ ",
                         paste(colnames(cem[-c(1, 5:6)]), collapse = "+"))
                   )
temp

f_model <- lm(formula(full), data = cem)
t_model <- lm(formula(temp), data = cem)
ols_mallows_cp(t_model, f_model) # 반드시 full_model 지정
```

-   Cp결과 값이 3.04로 나왔는데, 이는 t_model의 적합도와 변수의 수를 고려할 때의 평가 결과이다.
-   `t_model`의 경우 세 가지의 설명변수를 사용했는데 Cp값이 3.04가 나왔다는 건 적절하다고 보인다.

---   
    
-   이번에는 BIC 모형에서의 결과인, (t1, t2)를 사용한 모형에도 적용을 해보았다.
    
```{r}
full <- as.formula(paste("y ~ ",
                         paste(colnames(cem[-1]), collapse = "+"))
                         )
full

temp <- as.formula(paste("y ~ ",
                         paste(colnames(cem[-c(1, 4:6)]), collapse = "+"))
                   )
temp

f_model <- lm(formula(full), data = cem)
t_model <- lm(formula(temp), data = cem)
ols_mallows_cp(t_model, f_model) # 반드시 full_model 지정
```

-   Cp결과 값이 2.68로 나왔는데, 이는 t_model의 적합도와 변수의 수를 고려할 때의 평가 결과이다.
-   두 가지의 설명변수를 사용했는데 Cp값이 2.68가 나왔다는 건 세 가지의 설명변수와 비교했을 때 Cp값이 높게 나온 것으로 보아 과적합될 가능성도 염두를 해야할 필요가 있어 보인다.

---

# 중요한 기본 R함수 정리

-   변수명은 대문자 또는 소문자로 통일하는 것이 바람직하다.

## 데이터 확인
`str()`  
`head()`  
`tail()`  
`view()`  

## 회귀 분석
`stats::lm()`: lm(y ~ x, data = data)으로 지정  
-   `name()`는 모형 적합 객체(object)에 포함된 구성 요소들의 이름을 확인해준다.
    -   ex) model(dist_fit), model(summary(dist_fit))
-   이상점(outliers)의 진단
    -   `rstandard()`: 표준화 잔차(standardized residuals)
        -   표준화 잔차의 절대값이 2보다 크거나 같은 경우만을 선택
            -   rsd <- rstandard(dist_fit)
            -   outliers_d <- as.numeric(names(rsd)[abs(rsd) >= 2])
            -   dist[outliers_d, ]: 이상치 확인
            -   dist[-outliers_d, ]: 검토 결과 이를 제거해야 한다면 다음을 이용하여 제거
    -   `rstudent()`: 스튜던트화 잔차(studentized residuals)
        -   스튜던트화 잔차의 절대값이 3보다 크거나 같은 경우만을 선택
-   `ggexport()`을 이용하여 생성된 plot을 원하는 위치의 외부 화일로 저장가능하다.
    -   ggexport(object, filename = "./image/object.pdf")
-   `geom_smooth()`는 사전 분석 없이 기본 그림에 추정된 회귀식을 함께 도시해준다.

## 사용자 정의 함수 정리
`na_count(data)`: 결측값 확인    
`data_pre(data)`: 데이터의 변수들의 이름을 모두 소문자로 전환하는 함수  
`simple_reg(data, x, y)`: 단순 회귀 분석 최종 결과를 시각화하는 함수


---

  ⓒ Statistical Method1, Gachon University

---