{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd70bf5",
   "metadata": {
    "papermill": {
     "duration": 0.01075,
     "end_time": "2024-08-16T07:24:35.983041",
     "exception": false,
     "start_time": "2024-08-16T07:24:35.972291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"background-color:#d8ecff; color: #009dff;margin:0; display:inline-block;padding:.4rem;border-radius:.25rem;border:1px solid #009dff\">Importing Libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eee77a2",
   "metadata": {
    "papermill": {
     "duration": 2.705462,
     "end_time": "2024-08-16T07:24:38.698336",
     "exception": false,
     "start_time": "2024-08-16T07:24:35.992874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, os\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns|\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedShuffleSplit\n",
    "from autogluon.tabular import TabularPredictor\n",
    "# import optuna\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix, accuracy_score,matthews_corrcoef\n",
    "import scipy\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from autogluon.core.metrics import make_scorer\n",
    "warnings.filterwarnings('ignore')\n",
    "# Custom metric function\n",
    "def custom_mcc_metric(y_true, y_pred):\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return mcc\n",
    "custom_mcc_scorer = make_scorer(name='mcc',\n",
    "                                score_func=custom_mcc_metric,\n",
    "                                greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ef89b",
   "metadata": {
    "papermill": {
     "duration": 0.009945,
     "end_time": "2024-08-16T07:24:38.718437",
     "exception": false,
     "start_time": "2024-08-16T07:24:38.708492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "  ### <p style=\"background-color: #fdefff;color:#c12eff;display: inline-block;padding:.6rem;border-radius:.5rem;border: 1px solid #c059ff\">Loading data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a10a225-88a2-4b75-9840-5df12c5eb434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b00cf1",
   "metadata": {
    "papermill": {
     "duration": 15.322649,
     "end_time": "2024-08-16T07:24:54.050532",
     "exception": false,
     "start_time": "2024-08-16T07:24:38.727883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data : (3116945, 22)\n",
      "test_data : (2077964, 21)\n",
      "sample_submission_data : (2077964, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(os.path.join(os.getcwd(), \"kaggle/playground-series-s4e8/train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(os.getcwd(), \"kaggle/playground-series-s4e8/test.csv\"))\n",
    "sample_submission_data = pd.read_csv(os.path.join(os.getcwd(), \"kaggle/playground-series-s4e8/sample_submission.csv\"))\n",
    "\n",
    "print(\"train_data :\", train_data.shape)\n",
    "print(\"test_data :\", test_data.shape)\n",
    "print(\"sample_submission_data :\", sample_submission_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa22a9",
   "metadata": {
    "papermill": {
     "duration": 0.032892,
     "end_time": "2024-08-16T07:27:09.193886",
     "exception": false,
     "start_time": "2024-08-16T07:27:09.160994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"background-color:#d8ecff; color: #009dff;margin:0; display:inline-block;padding:.4rem;border-radius:.25rem;border:1px solid #009dff\">Handling NaN Values And Less Frequent Categories</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235db843-e8d3-44f3-9f01-637c93a21c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index(['id', 'class', 'cap-diameter', 'cap-shape', 'cap-surface', 'cap-color',\n",
    "#        'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
    "#        'stem-height', 'stem-width', 'stem-root', 'stem-surface', 'stem-color',\n",
    "#        'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color',\n",
    "#        'habitat', 'season', 'veil-info'],\n",
    "#       dtype='object')\n",
    "\n",
    "# 결측치와 특성 존재 여부를 동시에 다룰 수 있음: 앞에서 0과 1로 veil-type의 존재 여부를 표현하고, 뒤에서 'unknown'을 통해 veil-color의 결측치를 처리함으로써 두 정보를 효과적으로 결합할 수 있습니다.\n",
    "# 모델 학습에 유용한 정보 제공: 결측치나 특성의 존재 여부를 단순히 무시하지 않고, 이들을 결합하여 모델이 더 많은 패턴을 학습할 수 있게 도와줍니다.\n",
    "# Index(['w', 'y', 'n', 'u', 'k', 'e', 'g', 'p', 'r', 'o', 's', 'a', 't', 'd',\n",
    "#        'i', 'h', 'c', 'f', 'l', 'b', 'z', '8.25', '2.49', '3.32'],\n",
    "#       dtype='object', name='veil-color')\n",
    "# Index(['u', 'w', 'a', 'f', 'e', 'b', 'c', 'y', 'k', 'g', 'n', 's', 'r', 'd',\n",
    "#        'p', 'h', 'i', 'l', 'is None', 't', '21.11', '5.94'],\n",
    "#       dtype='object', name='veil-type')\n",
    "\n",
    "# Index(['f10', 'tnone', 't1', 't7', 't10', 't4', 't2', 't5', 't3', 't9',\n",
    "#        'fnone', 'f1', 'f2', 'f7', 'f4', 't8', 'f5', 'r2', 't6', 'f3', 'l4',\n",
    "#        't0', 'p5', 'z7', 'c10', 'x10', 'f9', 'f6', 's10', 'm9', 'hnone', 's1',\n",
    "#        'g3', 'g5', 'h7', 'e1', 'f0', 'r4', 'dnone', 's5', 'cnone', 'h1', 'p1',\n",
    "#        'h5', 'h10', 'w3', 'y2', 'a10', 'ynone', 'e10', 'p7', '10.310', 's2',\n",
    "#        'o2', 'g10', 'h2', 'g1', 's3', 'p3', 'knone', 'inone', 'nnone', 'rnone',\n",
    "#        'l5', 'c1', 'n10', 'c3', 'o10', 'e4', 'd10', 'f has-ring10', 'lnone',\n",
    "#        'c7', 'e3', 'y1', 'k10'],\n",
    "#       dtype='object', name='has-ring-type')\n",
    "\n",
    "train_data['veil-info'] = train_data['veil-type'].notna().astype(int).astype(str) + train_data['veil-color'].fillna('unknown')\n",
    "cap_shape_mapping = {'b': 0, 'c': 1, 'x': 2, 'f': 3, 's': 4, 'p': 5, 'o': 6}\n",
    "cap_color_mapping = {'n': 0, 'b': 1, 'g': 2, 'r': 3, 'p': 4, 'u': 5, 'e': 6, 'w': 7, 'y': 8, 'l': 9, 'o': 10, 'k': 11}\n",
    "ring_type_mapping = {'c': 0, 'e': 1, 'r': 2, 'g': 3, 'l': 4, 'p': 5, 's': 6, 'z': 7, 'y': 8, 'm': 9, 'f': 10}\n",
    "train_data['cap-shape'] = train_data['cap-shape'].map(cap_shape_mapping)\n",
    "train_data['cap-color'] = train_data['cap-color'].map(cap_color_mapping)\n",
    "train_data['has-ring-type'] = (\n",
    "    train_data['has-ring'] + \n",
    "    train_data['ring-type'].fillna('none').map(ring_type_mapping).apply(lambda x: f\"{x:.0f}\" if pd.notna(x) else 'none').str.replace('.0', '', regex=False)\n",
    ")\n",
    "\n",
    "test_data['veil-info'] = test_data['veil-type'].notna().astype(int).astype(str) + test_data['veil-color'].fillna('unknown')\n",
    "test_data['cap-shape'] = test_data['cap-shape'].map(cap_shape_mapping)\n",
    "test_data['cap-color'] = test_data['cap-color'].map(cap_color_mapping)\n",
    "test_data['has-ring-type'] = (\n",
    "    test_data['has-ring'] + \n",
    "    test_data['ring-type'].fillna('none').map(ring_type_mapping).apply(lambda x: f\"{x:.0f}\" if pd.notna(x) else 'none').str.replace('.0', '', regex=False)\n",
    ")\n",
    "train_data = train_data.drop(['id', 'veil-color', 'veil-type', 'has-ring', 'ring-type'], axis=1)\n",
    "test_data = test_data.drop(['id', 'veil-color', 'veil-type', 'has-ring', 'ring-type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ca3de9",
   "metadata": {
    "papermill": {
     "duration": 12.420604,
     "end_time": "2024-08-16T07:27:41.801476",
     "exception": false,
     "start_time": "2024-08-16T07:27:29.380872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    threshold = 100\n",
    "    \n",
    "    cat_feats = ['cap-shape', 'cap-surface', 'cap-color',\n",
    "       'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
    "       'stem-root', 'stem-surface', 'stem-color', 'spore-print-color',\n",
    "       'habitat', 'season', 'veil-info', 'has-ring-type']\n",
    "    \n",
    "    for feat in cat_feats:\n",
    "        if df[feat].dtype.name == 'category':\n",
    "            # Add 'missing' and 'noise' to categories if not present\n",
    "            if 'missing' not in df[feat].cat.categories:\n",
    "                df[feat] = df[feat].cat.add_categories('missing')\n",
    "            if 'noise' not in df[feat].cat.categories:\n",
    "                df[feat] = df[feat].cat.add_categories('noise')\n",
    "        else:\n",
    "            # Convert to category and add new categories\n",
    "            df[feat] = df[feat].astype('category')\n",
    "            df[feat] = df[feat].cat.add_categories(['missing', 'noise'])\n",
    "        \n",
    "        # Fill missing values with 'missing'\n",
    "        df[feat] = df[feat].fillna('missing')\n",
    "        \n",
    "        # Replace infrequent categories with 'noise'\n",
    "        counts = df[feat].value_counts(dropna=False)\n",
    "        infrequent_categories = counts[counts < threshold].index\n",
    "        df[feat] = df[feat].apply(lambda x: 'missing' if x in infrequent_categories else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "train_data = cleaning(train_data)\n",
    "test_data = cleaning(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cfd7ec-7938-440d-ad5d-60d349e068cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_features = ['stem-width', 'stem-height']\n",
    "group_means_train = train_data.groupby(group_by_features)['cap-diameter'].mean()\n",
    "\n",
    "def fill_na_with_group_mean(row):\n",
    "    if pd.isna(row['cap-diameter']):\n",
    "        group = tuple(row[group_by_features])\n",
    "        return group_means_train.get(group, np.nan) \n",
    "    else:\n",
    "        return row['cap-diameter']\n",
    "\n",
    "train_data['cap-diameter'] = train_data.apply(fill_na_with_group_mean, axis=1)\n",
    "test_data['cap-diameter'] = test_data.apply(fill_na_with_group_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9310106f-92d0-4ee5-9508-91cdff972390",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_features = ['stem-width', 'stem-height']\n",
    "group_means_train = train_data.groupby(group_by_features)['cap-diameter'].mean()\n",
    "group_by_features = ['stem-width', 'stem-height']\n",
    "\n",
    "# Calculate group means for the train data\n",
    "group_means_train = train_data.groupby(group_by_features)['cap-diameter'].mean()\n",
    "\n",
    "def fill_na_with_group_mean(row, group_means):\n",
    "    if pd.isna(row['cap-diameter']):\n",
    "        group = tuple(row[group_by_features])\n",
    "        return group_means.get(group, np.nan)\n",
    "    else:\n",
    "        return row['cap-diameter']\n",
    "\n",
    "# Apply to train_data using train group means\n",
    "train_data['cap-diameter'] = train_data.apply(fill_na_with_group_mean, axis=1, group_means=group_means_train)\n",
    "\n",
    "# Apply the same group means from train_data to test_data\n",
    "test_data['cap-diameter'] = test_data.apply(fill_na_with_group_mean, axis=1, group_means=group_means_train)\n",
    "###\n",
    "###\n",
    "# Calculate the mode from the training data\n",
    "cap_diameter_mode = train_data['cap-diameter'].mode()[0]\n",
    "stem_height_mode = train_data['stem-height'].mode()[0]\n",
    "\n",
    "# Fill missing values in the training data using the mode calculated from the training data\n",
    "train_data['cap-diameter'] = train_data['cap-diameter'].fillna(cap_diameter_mode)\n",
    "train_data['stem-height'] = train_data['stem-height'].fillna(stem_height_mode)\n",
    "\n",
    "# Fill missing values in the test data using the mode calculated from the training data\n",
    "test_data['cap-diameter'] = test_data['cap-diameter'].fillna(cap_diameter_mode)\n",
    "test_data['stem-height'] = test_data['stem-height'].fillna(stem_height_mode)\n",
    "\n",
    "cat_feats = ['cap-shape', 'cap-surface', 'cap-color',\n",
    "       'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
    "       'stem-root', 'stem-surface', 'stem-color', 'spore-print-color',\n",
    "       'habitat', 'season', 'veil-info', 'has-ring-type']\n",
    "\n",
    "for feat in cat_feats:\n",
    "    train_data[feat] = train_data[feat].astype('category')\n",
    "for feat in cat_feats:\n",
    "    test_data[feat] = test_data[feat].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48739aae",
   "metadata": {
    "papermill": {
     "duration": 0.030299,
     "end_time": "2024-08-16T07:29:18.447641",
     "exception": false,
     "start_time": "2024-08-16T07:29:18.417342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"background-color:#d8ecff; color: #009dff;margin:0; display:inline-block;padding:.4rem;border-radius:.25rem;border:1px solid #009dff\">Splitting Data</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85252d50-d480-4133-8b91-97da3c73d840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./kaggle/autogluon_models/\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #213-Ubuntu SMP Fri Aug 2 19:14:16 UTC 2024\n",
      "CPU Count:          32\n",
      "Memory Avail:       122.72 GB / 125.58 GB (97.7%)\n",
      "Disk Space Avail:   986.09 GB / 1829.69 GB (53.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2024-08-28 09:46:45,313\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-08-28 09:46:48,530\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"./kaggle/autogluon_models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Beginning AutoGluon training ... Time limit = 896s\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m AutoGluon will save models to \"./kaggle/autogluon_models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Train Data Rows:    2770617\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Train Data Columns: 18\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Label Column:       class\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tAvailable Memory:                    124563.41 MB\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTrain Data (Original)  Memory Usage: 103.06 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\t('category', []) : 15 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\t('category', []) : 15 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t2.0s = Fit runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t18 features in original data used to generate 18 features in processed data.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTrain Data (Processed) Memory Usage: 103.06 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Data preprocessing and feature engineering runtime = 2.42s ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Excluded models: ['XT', 'KNN', 'RF'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitting 90 L1 models ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 5.96s of the 893.66s of remaining time.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.67%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/10 [00:06<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBMXT_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9422\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.57s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 5.96s of the 885.15s of remaining time.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.67%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/10 [00:06<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBM_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9538\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 5.96s of the 876.62s of remaining time.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/10 [00:12<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.7585\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t11.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 5.96s of the 862.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭───────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetFastAI_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├───────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator          │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler            │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                       │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰───────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1' in 0.0133s.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(model_trial pid=5916)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 3ce3a582: FileNotFoundError('Could not fetch metrics for 3ce3a582: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/3ce3a582')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 742e8ba6: FileNotFoundError('Could not fetch metrics for 742e8ba6: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/742e8ba6')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 7471f804: FileNotFoundError('Could not fetch metrics for 7471f804: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetFastAI_BAG_L1/7471f804')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetFastAI_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 5.96s of the 849.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.88%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tStopping HPO to satisfy time limit...\n",
      "  0%|          | 0/10 [02:56<?, ?it/s]\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: XGBoost_BAG_L1/T1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.6779\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t175.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.61s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 5.96s of the 671.42s of remaining time.\n",
      "\u001b[36m(model_trial pid=6204)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭──────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetTorch_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├──────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator         │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler           │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                      │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰──────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1' in 0.0880s.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 4 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 4317b1c1: FileNotFoundError('Could not fetch metrics for 4317b1c1: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/4317b1c1')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 0cf20c47: FileNotFoundError('Could not fetch metrics for 0cf20c47: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/0cf20c47')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 2dd49251: FileNotFoundError('Could not fetch metrics for 2dd49251: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/2dd49251')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 03305fd9: FileNotFoundError('Could not fetch metrics for 03305fd9: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_BAG_L1/03305fd9')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5.96s of the 653.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.74%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9819\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_r177_BAG_L1 ... Tuning model for up to 5.96s of the 644.45s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r177_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_r177_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.7347\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r79_BAG_L1 ... Tuning model for up to 5.96s of the 637.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r79_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                          │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1' in 0.0060s.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 4 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 30d3af7f: FileNotFoundError('Could not fetch metrics for 30d3af7f: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/30d3af7f')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 2dde55ce: FileNotFoundError('Could not fetch metrics for 2dde55ce: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/2dde55ce')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 8f5dc9d3: FileNotFoundError('Could not fetch metrics for 8f5dc9d3: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/8f5dc9d3')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 6d6e3f90: FileNotFoundError('Could not fetch metrics for 6d6e3f90: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r79_BAG_L1/6d6e3f90')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r79_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBM_r131_BAG_L1 ... Tuning model for up to 5.96s of the 619.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r131_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.70%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBM_r131_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9359\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.89s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.61s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r191_BAG_L1 ... Tuning model for up to 5.96s of the 610.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r191_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=1.13%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Warning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1333, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/hpo/executors.py\", line 352, in validate_search_space\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=15998, ip=172.17.0.2)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 332, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2221, in _train_single_full\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1501, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 184, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1335, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 558, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_r9_BAG_L1 ... Tuning model for up to 5.96s of the 603.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r9_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.69%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_r9_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9183\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.53s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBM_r96_BAG_L1 ... Tuning model for up to 5.96s of the 596.08s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r96_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.67%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBM_r96_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.7686\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r22_BAG_L1 ... Tuning model for up to 5.96s of the 587.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r22_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                          │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1' in 0.0054s.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 2aec5b08: FileNotFoundError('Could not fetch metrics for 2aec5b08: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/2aec5b08')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 508638fe: FileNotFoundError('Could not fetch metrics for 508638fe: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/508638fe')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - e6a492e9: FileNotFoundError('Could not fetch metrics for e6a492e9: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r22_BAG_L1/e6a492e9')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r22_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: XGBoost_r33_BAG_L1 ... Tuning model for up to 5.96s of the 569.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r33_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.92%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: XGBoost_r33_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.387\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t218.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.53s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_r137_BAG_L1 ... Tuning model for up to 5.96s of the 348.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r137_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.67%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_r137_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.6853\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r102_BAG_L1 ... Tuning model for up to 5.96s of the 340.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r102_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=1.13%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Warning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1333, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/hpo/executors.py\", line 352, in validate_search_space\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=22180, ip=172.17.0.2)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 332, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2221, in _train_single_full\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1501, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 184, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1335, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 558, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_r13_BAG_L1 ... Tuning model for up to 5.96s of the 333.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r13_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.70%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_r13_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.7895\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t5.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBM_r188_BAG_L1 ... Tuning model for up to 5.96s of the 325.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r188_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBM_r188_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9864\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.61s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r145_BAG_L1 ... Tuning model for up to 5.96s of the 316.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r145_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=1.13%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Warning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1333, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_executor.validate_search_space(search_space, self.name)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/hpo/executors.py\", line 352, in validate_search_space\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.hpo.exceptions.EmptySearchSpace\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ray.exceptions.RayTaskError(TimeLimitExceeded): \u001b[36mray::_ray_fit()\u001b[39m (pid=24538, ip=172.17.0.2)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 332, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2221, in _train_single_full\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1501, in hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 184, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, hpo_executor=hpo_executor, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 1335, in _hyperparameter_tune\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return skip_hpo(X=X, y=y, X_val=X_val, y_val=y_val, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 122, in skip_hpo\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fit_and_save_model(model=model, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_val=y_val, time_start=time.time(), time_limit=time_limit)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/model_trial.py\", line 96, in fit_and_save_model\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     model.fit(**fit_args, time_limit=time_left)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m   File \"/usr/local/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 558, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m     raise TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: XGBoost_r89_BAG_L1 ... Tuning model for up to 5.96s of the 310.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for XGBoost_r89_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.88%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: XGBoost_r89_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.3934\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t250.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t7.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r30_BAG_L1 ... Tuning model for up to 5.96s of the 58.0s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r30_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                          │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1' in 0.0075s.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 4 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 06bab833: FileNotFoundError('Could not fetch metrics for 06bab833: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/06bab833')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - dc09502d: FileNotFoundError('Could not fetch metrics for dc09502d: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/dc09502d')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 3904529c: FileNotFoundError('Could not fetch metrics for 3904529c: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/3904529c')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 381f4faf: FileNotFoundError('Could not fetch metrics for 381f4faf: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r30_BAG_L1/381f4faf')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r30_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: LightGBM_r130_BAG_L1 ... Tuning model for up to 5.96s of the 39.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for LightGBM_r130_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: LightGBM_r130_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.9876\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t6.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetTorch_r86_BAG_L1 ... Tuning model for up to 5.96s of the 30.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m ╭──────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Configuration for experiment     NeuralNetTorch_r86_BAG_L1   │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ├──────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Search algorithm                 SearchGenerator             │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Scheduler                        FIFOScheduler               │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m │ Number of trials                 10                          │\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m ╰──────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \n",
      "\u001b[36m(_dystack pid=436)\u001b[0m View detailed results here: /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m Reached timeout of 5.956292346239089 seconds. Stopping all trials.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Wrote the latest version of all result files and experiment state to '/workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1' in 0.0054s.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Failed to fetch metrics for 3 trial(s):\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - bbda1ffc: FileNotFoundError('Could not fetch metrics for bbda1ffc: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/bbda1ffc')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 17843940: FileNotFoundError('Could not fetch metrics for 17843940: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/17843940')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m - 875313f3: FileNotFoundError('Could not fetch metrics for 875313f3: both result.json and progress.csv were not found at /workspace/data/kaggle/autogluon_models/ds_sub_fit/sub_fit_ho/models/NeuralNetTorch_r86_BAG_L1/875313f3')\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m No model was trained during hyperparameter tuning NeuralNetTorch_r86_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: CatBoost_r50_BAG_L1 ... Tuning model for up to 5.96s of the 12.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for CatBoost_r50_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=0.70%)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Fitted model: CatBoost_r50_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.7596\t = Validation score   (f1_weighted)\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t4.49s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m Hyperparameter tuning model: NeuralNetFastAI_r11_BAG_L1 ... Tuning model for up to 5.96s of the 5.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tNo hyperparameter search space specified for NeuralNetFastAI_r11_BAG_L1. Skipping HPO. Will train one model based on the provided hyperparameters.\n",
      "\u001b[36m(_dystack pid=436)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=6, gpus=0, memory=1.13%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize TabularPredictor with the custom MCC metric\n",
    "predictor = TabularPredictor(\n",
    "    label='class',\n",
    "    problem_type='binary',\n",
    "    eval_metric='f1_weighted', #custom_mcc_scorer, 'f1_weighted', 'log_loss'\n",
    "    path='./kaggle/autogluon_models/',\n",
    "    verbosity=2,\n",
    ")\n",
    "predictor.fit(train_data,\n",
    "            presets='best_quality', # 'high_quality\n",
    "            # time_limit=3600,  # Set an overall time limit\n",
    "            # num_bag_folds=10,  # Use 5-fold bagging\n",
    "            # num_stack_levels=2,  # Use 2 levels of stacking\n",
    "            refit_full=True,  # Refit the best model on the full dataset\n",
    "            # hyperparameters={\n",
    "            #     'GBM': {'num_boost_round': 100, 'early_stopping_rounds': 10},\n",
    "            #     'CAT': {'iterations': 1000, 'early_stopping_rounds': 50},\n",
    "            #     'XGB': {'n_estimators': 100, 'early_stopping_rounds': 10},\n",
    "            #     'FASTAI': {'epochs': 100, 'early_stopping_patience': 10}\n",
    "            # },\n",
    "            hyperparameter_tune_kwargs={\n",
    "                'num_trials': 10,  # Set this based on resource availability\n",
    "                'scheduler': 'local',\n",
    "                'searcher': 'auto'\n",
    "            },\n",
    "            excluded_model_types=['KNN', 'XT', 'RF', 'LR'],  # Exclude KNN and Extra Trees models\n",
    "            save_space=True  # Save space by removing intermediate models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1dff2-c34f-469a-a505-5162e16dde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = sample_submission_data.pop('id')\n",
    "y_test_pred = predictor.predict(test_data)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': id_column,\n",
    "    'class': y_test_pred\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('./kaggle/submission_autog_0827_1.csv', index=False)\n",
    "print(\"Submission file created: submission5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a0259-75ad-456c-9faa-988e8539cfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 344.562172,
   "end_time": "2024-08-16T07:30:17.540225",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-16T07:24:32.978053",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
